{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "from data_loader.data_generator import MNISTDataGenerator, LinearDataGenerator\n",
    "from models.npmodels import NPModel4,DirectNPModel4,AENPModel,AEDFANPModel\n",
    "from trainers.sf_trainer import SFTrainer, AESFTrainer\n",
    "from utils.config import process_config\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l, tf_eigvecs, tf_eigvals\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#p = self.config.state_size[0]\n",
    "p=784# inshape \n",
    "m =1000# hiddenshap\n",
    "j = 10#outshpae\n",
    "#n = 10\n",
    "var_xi = 0.1\n",
    "learning_rate=4e-6\n",
    "lmda_learning_rate=6.3e-5\n",
    "#lmda_learning_rate=0\n",
    "#Training data inputs\n",
    "x=tf.placeholder(tf.float32,[None,p], name = 'x')\n",
    "y=tf.placeholder(tf.float32,[None,j], name = 'y')\n",
    "\n",
    "#Scale weight initialization\n",
    "alpha0 = np.sqrt(2.0/p)\n",
    "alpha1 = np.sqrt(2.0/m)\n",
    "alpha2 = np.sqrt(2.0/j)\n",
    "alpha3 = 1\n",
    "\n",
    "A = tf.Variable(rng.randn(p+1,m)*alpha0, name=\"hidden_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(m+1,j)*alpha1, name=\"output_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(m+1,j)*alpha1, name=\"feedback_weights\", dtype=tf.float32)\n",
    "\n",
    "# network architecture with ones added for bias terms\n",
    "#0 = tf.ones([batch_size, 1], tf.float32)\n",
    "#1 = tf.ones([batch_size, 1], tf.float32)\n",
    "e0 = tf.ones([tf.shape(x)[0], 1], tf.float32)\n",
    "e1 = tf.ones([tf.shape(x)[0], 1], tf.float32)\n",
    "# e0 = tf.ones([1,batch_size], tf.float32)\n",
    "# e1 = tf.ones([1,batch_size], tf.float32)\n",
    "\n",
    "x_aug = tf.concat([x, e0], 1)\n",
    "h = tf.sigmoid(tf.matmul(x_aug, A))\n",
    "#Make some noise\n",
    "h_aug = tf.concat([h, e1], 1)\n",
    "xi = tf.random_normal(shape=tf.shape(h_aug), mean=0.0, stddev=var_xi, dtype=tf.float32)\n",
    "h_tilde = h_aug + xi\n",
    "#Add noise to hidden layer\n",
    "#y_p = tf.sigmoid(tf.matmul(h_tilde, W))\n",
    "y_p = tf.matmul(h_tilde, W)\n",
    "y_p_0 = tf.matmul(h_aug, W)\n",
    "\n",
    "trainable = [A, W, B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'strided_slice_2:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error\n",
    "loss = tf.reduce_sum(tf.pow(y_p-y, 2))/2\n",
    "loss_0 = tf.reduce_sum(tf.pow(y_p_0-y, 2))/2\n",
    "e = (y_p - y)\n",
    "\n",
    "h_prime = tf.multiply(h_tilde, 1-h_tilde)[:,0:m]\n",
    "\n",
    "#Feedback data for saving\n",
    "#Only take first item in epoch\n",
    "delta_bp = tf.matmul(e, tf.transpose(W[0:m,:]))[0,:]\n",
    "delta_fa = tf.matmul(e, tf.transpose(B[0:m,:]))[0,:]\n",
    "norm_W = tf.norm(W)\n",
    "norm_B = tf.norm(B)\n",
    "error_FA = tf.norm(delta_bp - delta_fa)\n",
    "alignment = tf.reduce_sum(tf.multiply(delta_fa,delta_bp))/tf.norm(delta_fa)/tf.norm(delta_bp)\n",
    "norm_diff = tf.norm(W - B)\n",
    "eigs = tf_eigvals(tf.matmul(tf.transpose(B), W))\n",
    "\n",
    "#Compute updates for W and A (based on B)\n",
    "#Node pert\n",
    "lmda = tf.matmul(e, tf.transpose(B[0:m,:]))\n",
    "#Backprop\n",
    "#lmda = tf.matmul(e, tf.transpose(W[0:m,:]))\n",
    "grad_W = tf.gradients(xs=W, ys=loss)[0]\n",
    "grad_A = tf.matmul(tf.transpose(x_aug), tf.multiply(h_prime, lmda))\n",
    "grad_B = tf.matmul(tf.matmul(B, tf.transpose(e)) - tf.transpose(xi)*(loss - loss_0)/var_xi, e)\n",
    "\n",
    "new_W = W.assign(W - learning_rate*grad_W)\n",
    "new_A = A.assign(A - learning_rate*grad_A)            \n",
    "new_B = B.assign(B - lmda_learning_rate\n",
    "                 *grad_B)            \n",
    "train_step = [new_W, new_A, new_B]\n",
    "correct_prediction = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#Also need to add eigenvector stuff\n",
    "training_metrics = [alignment, norm_W, norm_B, error_FA, eigs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH No:0 completed\n",
      "EPOCH No:1 completed\n",
      "EPOCH No:2 completed\n",
      "EPOCH No:3 completed\n",
      "EPOCH No:4 completed\n",
      "EPOCH No:5 completed\n",
      "EPOCH No:6 completed\n",
      "EPOCH No:7 completed\n",
      "EPOCH No:8 completed\n",
      "EPOCH No:9 completed\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "iteration= 100000\n",
    "epoch=10\n",
    "store_al=np.zeros((epoch,iteration))\n",
    "store_df=np.zeros((epoch,iteration))\n",
    "store_err=np.zeros((epoch,iteration))\n",
    "store_acc=np.zeros((epoch,iteration))\n",
    "# store_out=np.zeros((N, 4))\n",
    "# x_in=[[0,0],[0,1],[1,0],[1,1]]\n",
    "batch_size=50\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    for epoch_no in range(epoch):\n",
    "        for idx in range(iteration):\n",
    "            (train_x, train_y) = mnist.train.next_batch(batch_size) \n",
    "            _,align,diff,err,acc=sess.run([train_step,alignment,norm_diff,loss_0,accuracy],feed_dict={x: train_x, y: train_y})\n",
    "            \n",
    "            if np.isnan(err)==True:\n",
    "        \n",
    "                print(\"\\n\\tModel does not converge!!!\\n\")\n",
    "                break\n",
    "#             store_out[idx,:] = out[0][:,0]\n",
    "            store_err[epoch_no,idx]=err\n",
    "            store_al[epoch_no,idx]=align\n",
    "            store_acc[epoch_no,idx]=acc\n",
    "        print(\"EPOCH No:%d completed\"%epoch_no)\n",
    "#             store_df.append(diff)\n",
    "        #print(align)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"MNIST_np.pkl\",'wb') as f:\n",
    "    pickle.dump([store_err,store_al,store_acc,iteration,epoch],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
