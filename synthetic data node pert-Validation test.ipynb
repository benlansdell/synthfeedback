{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "from data_loader.data_generator import MNISTDataGenerator, LinearDataGenerator\n",
    "from models.npmodels import NPModel4,DirectNPModel4,AENPModel,AEDFANPModel\n",
    "from trainers.sf_trainer import SFTrainer, AESFTrainer\n",
    "from utils.config import process_config\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import itertools\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l, tf_eigvecs, tf_eigvals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "from plotly.graph_objs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inshape=30\n",
    "hidden=20\n",
    "outshape=10\n",
    "batch_size=50\n",
    "T = rng.randn(outshape, inshape)\n",
    "def traindata(T, batch_size):\n",
    "    train_x = rng.randn( batch_size,T.shape[1])\n",
    "    train_y = np.dot( train_x,T.T)\n",
    "    return (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#p = self.config.state_size[0]\n",
    "p=30# inshape \n",
    "m =20# hiddenshap\n",
    "j = 10#outshpae\n",
    "#n = 10\n",
    "var_xi = 0.1\n",
    "# learning_rate=0\n",
    "# lmda_learning_rate=1e-5\n",
    "#lmda_learning_rate=0\n",
    "\n",
    "#Training data inputs\n",
    "x=tf.placeholder(tf.float32,[None,p], name = 'x')\n",
    "y=tf.placeholder(tf.float32,[None,j], name = 'y')\n",
    "learning_rate=tf.placeholder(tf.float32,[None],name='learning_rate')\n",
    "lmda_learning_rate=tf.placeholder(tf.float32,[None],name='lmda_learning_rate')\n",
    "\n",
    "#Scale weight initialization\n",
    "alpha0 = np.sqrt(2.0/p)\n",
    "alpha1 = np.sqrt(2.0/m)\n",
    "alpha2 = np.sqrt(2.0/j)\n",
    "alpha3 = 1\n",
    "\n",
    "A = tf.Variable(rng.randn(p+1,m)*alpha0, name=\"hidden_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(m+1,j)*alpha1, name=\"output_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(m+1,j)*alpha2, name=\"feedback_weights\", dtype=tf.float32)\n",
    "\n",
    "# network architecture with ones added for bias terms\n",
    "e0 = tf.ones([batch_size, 1], tf.float32)\n",
    "e1 = tf.ones([batch_size, 1], tf.float32)\n",
    "x_aug = tf.concat([x, e0], 1)\n",
    "h = tf.sigmoid(tf.matmul(x_aug, A))\n",
    "#Make some noise\n",
    "h_aug = tf.concat([h, e1], 1)\n",
    "xi = tf.random_normal(shape=tf.shape(h_aug), mean=0.0, stddev=var_xi, dtype=tf.float32)\n",
    "h_tilde = h_aug + xi\n",
    "#Add noise to hidden layer\n",
    "y_p = tf.matmul(h_tilde, W)\n",
    "y_p_0 = tf.matmul(h_aug, W)\n",
    "\n",
    "trainable = [A, W, B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error\n",
    "loss = tf.reduce_sum(tf.pow(y_p-y, 2))/2\n",
    "loss_0 = tf.reduce_sum(tf.pow(y_p_0-y, 2))/2\n",
    "e = (y_p - y)\n",
    "h_prime = tf.multiply(h_tilde, 1-h_tilde)[:,0:m]\n",
    "\n",
    "#Feedback data for saving\n",
    "#Only take first item in epoch\n",
    "delta_bp = tf.matmul(e, tf.transpose(W[0:m,:]))[0,:]\n",
    "delta_fa = tf.matmul(e, tf.transpose(B[0:m,:]))[0,:]\n",
    "norm_W = tf.norm(W)\n",
    "norm_B = tf.norm(B)\n",
    "error_FA = tf.norm(delta_bp - delta_fa)\n",
    "alignment = tf.reduce_sum(tf.multiply(delta_fa,delta_bp))/tf.norm(delta_fa)/tf.norm(delta_bp)\n",
    "norm_diff = tf.norm(W - B)\n",
    "eigs = tf_eigvals(tf.matmul(tf.transpose(B), W))\n",
    "\n",
    "#Compute updates for W and A (based on B)\n",
    "lmda = tf.matmul(e, tf.transpose(B[0:m,:]))\n",
    "grad_W = tf.gradients(xs=W, ys=loss)[0]\n",
    "grad_A = tf.matmul(tf.transpose(x_aug), tf.multiply(h_prime, lmda))\n",
    "grad_B = tf.matmul(tf.matmul(B, tf.transpose(e)) - tf.transpose(xi)*(loss - loss_0)/var_xi, e)\n",
    "\n",
    "new_W = W.assign(W - learning_rate*grad_W)\n",
    "new_A = A.assign(A - learning_rate*grad_A)            \n",
    "new_B = B.assign(B - lmda_learning_rate*grad_B)\n",
    "train_step = [new_W, new_A, new_B]\n",
    "\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# #Also need to add eigenvector stuff\n",
    "# training_metrics = [alignment, norm_W, norm_B, error_FA, eigs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(store_err[1,1])\n",
    "# store_err[np.isnan(store_err)]=0\n",
    "# plt.imshow(store_err[0], cmap='hot',interpolation='nearest')\n",
    "with open('Synthdata_nodepert.pkl', 'wb') as f:\n",
    "    pickle.dump(store_err, f)\n",
    "    pickle.dump(store_al,f)\n",
    "    pickle.dump(store_df,f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present combo: 1\n",
      "Present learning rate index no: 0.001\n",
      "Present lmda learning rate index no: 1e-05\n",
      "Present combo: 2\n",
      "Present learning rate index no: 0.001\n",
      "Present lmda learning rate index no: 6.30957344480193e-06\n",
      "Present combo: 3\n",
      "Present learning rate index no: 0.001\n",
      "Present lmda learning rate index no: 3.981071705534969e-06\n",
      "Present combo: 4\n",
      "Present learning rate index no: 0.001\n",
      "Present lmda learning rate index no: 2.5118864315095823e-06\n",
      "Present combo: 5\n",
      "Present learning rate index no: 0.001\n",
      "Present lmda learning rate index no: 1.584893192461114e-06\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "eta_1=np.logspace(-3,-4,6)\n",
    "# eta_1=[0.1,0.1,0.1,0.1,0.1,0.1]\n",
    "eta_lmda=np.logspace(-5,-6,6)\n",
    "combo=list(itertools.product(eta_1,eta_lmda))\n",
    "iteration=2000\n",
    "store_al=[[] for i in range(len(combo))]\n",
    "store_df=[[] for i in range(len(combo))]\n",
    "store_err=[[] for i in range(len(combo))]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(len(combo)):\n",
    "        print(\"Present combo:\",i+1)\n",
    "        print(\"Present learning rate index no:\",combo[i][0])\n",
    "        learning_rate1=[combo[i][0]]\n",
    "        lmda_learning_rate1=[combo[i][1]]\n",
    "        print(\"Present lmda learning rate index no:\",combo[i][1])  \n",
    "        for idx in range(iteration):\n",
    "            (train_x, train_y) = traindata(T, batch_size) \n",
    "    \n",
    "            \n",
    "            _,align,diff,err=sess.run([train_step,alignment,norm_diff,loss_0],feed_dict={x: train_x, y: train_y,\n",
    "                                                                                         learning_rate:learning_rate1,\n",
    "                                                                                         lmda_learning_rate:lmda_learning_rate1})\n",
    "            store_al[i].append(align)\n",
    "            store_df[i].append(diff)\n",
    "            store_err[i].append(err)\n",
    "#             if (idx+1)%500==0:\n",
    "#                 print(\"Iteration count:\",idx+1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(store_err)\n",
    "plt.show()\n",
    "plt.pcolor(store_err)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(store_err)):\n",
    "    leg='Combination '+str(i+1)\n",
    "    plt.plot(store_err[i],label=leg)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.001     , 0.00063096, 0.00039811, 0.00025119, 0.00015849,\n",
       "       0.0001    ])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
