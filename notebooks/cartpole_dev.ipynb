{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lansdell/projects/synthfeedback\n"
     ]
    }
   ],
   "source": [
    "cd /home/lansdell/projects/synthfeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from numpy import random as rng\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backprop\n",
      "For 1000 epochs\n",
      "Learning rate: 0.001000\n",
      "Lambda learning rate: 0.000050\n",
      "Variance xi: 0.500000\n",
      "Saving results: 0\n"
     ]
    }
   ],
   "source": [
    "#method = args.method\n",
    "#save = bool(args.save)\n",
    "\n",
    "method = 'backprop'\n",
    "save = False\n",
    "\n",
    "anneal = True\n",
    "\n",
    "# Global config variables\n",
    "num_steps = 10 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 20\n",
    "in_dim = 4\n",
    "state_size = 50\n",
    "learning_rate = 1e-3\n",
    "alpha2 = 1\n",
    "activation = tf.tanh\n",
    "act_prime = lambda x: 1 - tf.multiply(x,x)\n",
    "\n",
    "#Node pert params\n",
    "lmbda = 5e-5\n",
    "var_xi = 0.5\n",
    "p_fire = 0.1 #prob of firing\n",
    "\n",
    "acclimatize = True\n",
    "\n",
    "grad_max = 10\n",
    "\n",
    "N_epochs = 1000\n",
    "N_episodes = 10\n",
    "\n",
    "report_rate = 100\n",
    "fn_out = './experiments/cartpole_rnn_np/%s_learning_rate_%f_lmbda_%f_varxi_%f.npz'%(method, learning_rate, lmbda, var_xi)\n",
    "\n",
    "#Things to save with output\n",
    "params = {\n",
    "'num_steps': num_steps,\n",
    "'batch_size': batch_size,\n",
    "'in_dim': in_dim,\n",
    "'state_size': state_size,\n",
    "'learning_rate': learning_rate,\n",
    "'alpha2': alpha2,\n",
    "'lmbda': lmbda,\n",
    "'var_xi': var_xi,\n",
    "'p_fire': p_fire,\n",
    "'grad_max': grad_max,\n",
    "'N_epochs': N_epochs,\n",
    "'N_episodes': N_episodes,\n",
    "'acclimatize':acclimatize\n",
    "}\n",
    "\n",
    "#method = 'backprop'\n",
    "#method = 'feedbackalignment'\n",
    "#method = 'nodepert'\n",
    "#method = 'weightsym'\n",
    "\n",
    "print(\"Using %s\"%method)\n",
    "print(\"For %d epochs\"%N_epochs)\n",
    "print(\"Learning rate: %f\"%learning_rate)\n",
    "print(\"Lambda learning rate: %f\"%lmbda)\n",
    "print(\"Variance xi: %f\"%var_xi)\n",
    "print(\"Saving results: %d\"%save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell(rnn_input, state, W, U):\n",
    "    ones0 = tf.ones([batch_size, 1], tf.float32)\n",
    "    state_p = tf.concat([state, ones0], 1)\n",
    "    return activation(tf.matmul(rnn_input, U) + tf.matmul(state_p, W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_episodes, num_steps, state_size=state_size, verbose=True):\n",
    "    xs = np.zeros((N_epochs, num_episodes, num_steps, batch_size, in_dim))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        alignments = []\n",
    "\n",
    "        for idx in range(N_epochs):\n",
    "            print(\"Epoch: %d\"%idx)\n",
    "            #if idx < 4 and acclimatize:\n",
    "            #    ts = train_step_B\n",
    "            #else:\n",
    "            ts = train_step\n",
    "            training_loss = 0\n",
    "            training_x = np.zeros((batch_size, in_dim))\n",
    "            #training_x = 2*rng.randn(batch_size, in_dim)\n",
    "            #training_x[:,1] = np.pi\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            for step in range(num_episodes):\n",
    "                tr_init_gradW = np.zeros((state_size+1, state_size))\n",
    "                tr_init_gradB = np.zeros((state_size, state_size))\n",
    "                tr_init_gradU = np.zeros((in_dim, state_size))\n",
    "                tr_loss, tr_losses, training_loss_, training_state, training_x, _, align, x_o = \\\n",
    "                    sess.run([loss, losses, total_loss, final_state, final_x, ts, aments, rnn_inputs], \\\n",
    "                                  feed_dict={init_state:training_state, init_x: training_x, \\\n",
    "                                  init_gradU: tr_init_gradU, init_gradW: tr_init_gradW, \\\n",
    "                                  init_gradB: tr_init_gradB})\n",
    "                #print(np.array(x_o).shape)\n",
    "                xs[idx, step, :, :, :] = np.array(x_o)[:,:,:]\n",
    "                training_loss += training_loss_\n",
    "                if step % report_rate == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step %d for last %d steps: %f\"%(step, report_rate, \\\n",
    "                                                                               training_loss/report_rate))\n",
    "                    training_losses.append(training_loss/report_rate)\n",
    "                    alignments.append(align)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses, step, alignments, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "## BACKPROP ##\n",
    "##############\n",
    "\n",
    "def backprop():\n",
    "\n",
    "    grad_U = init_gradU\n",
    "    grad_W = init_gradW\n",
    "    grad_B = init_gradB\n",
    "    alnments = []\n",
    "    for i in range(num_steps):\n",
    "        for j in range(i+1)[::-1]:\n",
    "            if j == i:\n",
    "                delta = tf.multiply(tf.matmul(delta0s[i][:,None],tf.transpose(V[0:state_size,:])), \\\n",
    "                                    act_prime(rnn_outputs[j]))\n",
    "            else:\n",
    "                delta = tf.multiply(tf.matmul(delta, tf.transpose(W[0:state_size,:])), act_prime(rnn_outputs[j]))\n",
    "            grad_U = grad_U + tf.matmul(tf.transpose(rnn_inputs[j]), delta)\n",
    "            if j > 0:\n",
    "                grad_W = grad_W + tf.matmul(tf.transpose(tf.concat([rnn_outputs[j-1], ones0],1)), delta)\n",
    "\n",
    "    grad_V = tf.gradients(xs=V, ys=total_loss)[0]\n",
    "    #grad_W = tf.gradients(xs=W, ys=total_loss)[0]\n",
    "    #grad_U = tf.gradients(xs=U, ys=total_loss)[0]\n",
    "\n",
    "    return grad_U, grad_W, grad_B, grad_V, alnments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if method == 'backprop':\n",
    "    trainer = backprop\n",
    "elif method == 'feedbackalignment':\n",
    "    trainer = feedbackalignment\n",
    "elif method == 'nodepert':\n",
    "    trainer = nodepert\n",
    "elif method == 'weightsym':\n",
    "    trainer = weightsym\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "init_x = tf.zeros([batch_size, in_dim], dtype=np.float32)\n",
    "init_state = tf.zeros([batch_size, state_size], dtype=np.float32)\n",
    "init_gradW = tf.zeros([state_size+1, state_size], dtype=np.float32)\n",
    "init_gradB = tf.zeros([state_size, state_size], dtype=np.float32)\n",
    "init_gradU = tf.zeros([in_dim, state_size], dtype=np.float32)\n",
    "alignment = tf.zeros([in_dim, state_size], dtype=np.float32)\n",
    "\n",
    "ones0 = tf.ones([batch_size, 1], tf.float32)\n",
    "#U = tf.get_variable('U', [in_dim, state_size])\n",
    "#W = tf.get_variable('W', [state_size+1, state_size])\n",
    "#V = tf.get_variable('V', [state_size+1, 1])\n",
    "U = tf.Variable(rng.randn(in_dim, state_size)*alpha2, name=\"input_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(state_size+1, state_size)*alpha2, name=\"feedforward_weights\", dtype=tf.float32)\n",
    "#W = tf.Variable(np.zeros((state_size+1, state_size))*alpha2, name=\"feedforward_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(state_size, state_size)*alpha2, name=\"feedback_weights\", dtype=tf.float32)\n",
    "V = tf.Variable(rng.randn(state_size+1, 1)*alpha2, name=\"output_weights\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pole task and dynamics here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = init_x\n",
    "state = init_state\n",
    "state_p = init_state\n",
    "rnn_inputs = []\n",
    "rnn_outputs = []\n",
    "rnn_pert_outputs = []\n",
    "noise_outputs = []\n",
    "heights = []\n",
    "hs = []\n",
    "actions = []\n",
    "\n",
    "#Cart pole dynamics simulated for num_steps \n",
    "m = 1.1\n",
    "mp = 0.1\n",
    "g = 9.8\n",
    "l = 0.5\n",
    "tau = 0.04\n",
    "Fmax = 10\n",
    "max_h = 3\n",
    "gamma = 10\n",
    "\n",
    "#Equations of motion:\n",
    "#theta_dd = (m*g*sin(theta) - cos(theta)*(F + mp*l*theta_d*theta_d*sin(theta)))/((4/3)*m*l - mp*l*cos(theta)*cos(theta))\n",
    "#theta_d += tau*theta_dd\n",
    "#theta += tau*theta\n",
    "#h_dd = (F + mp*l*(theta_d*theta_d*sin(theta)-theta_dd*cos(theta)))/m\n",
    "#h_d += tau*h_dd\n",
    "#h += tau*h_d\n",
    "\n",
    "for idx in range(num_steps):\n",
    "    mask = tf.random_uniform(state.shape) < p_fire\n",
    "    xi = tf.multiply(tf.random_normal(state.shape)*var_xi, tf.to_float(mask))\n",
    "    phi = tf.random_normal((batch_size,1))*Fmax/5\n",
    "    #Compute new state\n",
    "    state = rnn_cell(x, state, W, U)\n",
    "    state_p = rnn_cell(x, state_p, W, U) + xi\n",
    "    #Compute action\n",
    "    action = tf.matmul(tf.concat([state, ones0], 1), V)\n",
    "    F = tf.squeeze(Fmax*activation(action) + phi)\n",
    "    #Compute new x\n",
    "    theta_dd = (m*g*tf.sin(x[:,1]) - tf.cos(x[:,1])*(F + mp*l*x[:,0]*x[:,0]*tf.sin(x[:,1])))/((4/3)*m*l -\\\n",
    "                mp*l*tf.cos(x[:,1])*tf.cos(x[:,1]))\n",
    "    h_dd = (F + mp*l*(x[:,0]*x[:,0]*tf.sin(x[:,1])-theta_dd*tf.cos(x[:,1])))/m\n",
    "    \n",
    "    #h_dd = (F - mp*l*x[:,0]*x[:,0]*tf.sin(x[:,1]) + mp*g*tf.sin(x[:,1])*tf.cos(x[:,1]))/(m - mp*tf.cos(x[:,1])*tf.cos(x[:,1]))\n",
    "    #theta_dd = (h_dd*tf.cos(x[:,1]) + g*tf.sin(x[:,1]))/l \n",
    "\n",
    "    x_list = []\n",
    "    x_list.append(x[:,0] + tau*theta_dd)   #x0 = theta_dot\n",
    "    x_list.append(x[:,1] + tau*x[:,0])     #x1 = theta\n",
    "    x_list.append(x[:,2] + tau*h_dd)       #x2 = h_dot\n",
    "    x_list.append(x[:,3] + tau*x[:,2])     #x3 = h\n",
    "    #x_list.append(tf.clip_by_value(x[:,3] + tau*x[:,2], -4*max_h, 4*max_h))     #x3 = h\n",
    "    x = tf.stack(x_list, axis = 1)\n",
    "    #height = tf.cos(x[:,1])\n",
    "    height = x[:,1]\n",
    "    heights.append(height)\n",
    "    hs.append(x[:,2])\n",
    "    rnn_inputs.append(x)\n",
    "    rnn_outputs.append(state)\n",
    "    rnn_pert_outputs.append(state_p)\n",
    "    noise_outputs.append(xi)\n",
    "    actions.append(action)\n",
    "    \n",
    "final_x = rnn_inputs[-1]\n",
    "final_state = rnn_outputs[-1]\n",
    "\n",
    "#Define loss function....\n",
    "#loss = \n",
    "\n",
    "loss = [gamma*tf.pow(height, 2)/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "losses = [gamma*tf.reduce_sum(tf.pow(height, 2))/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "#Perturbed outputs and loss\n",
    "loss_pert = [gamma*tf.pow(height, 2)/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "losses_pert = [gamma*tf.reduce_sum(tf.pow(height, 2))/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "total_loss_pert = tf.reduce_mean(losses_pert)\n",
    "\n",
    "#e0s = [(height+1) for height in heights]\n",
    "#delta0s = e0s\n",
    "\n",
    "e0s = [tf.gradients(xs=actn, ys=lo)[0][:,0] for (actn,lo) in zip(actions, loss)]\n",
    "delta0s = e0s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_U, grad_W, grad_B, grad_V, aments = trainer()\n",
    "\n",
    "new_B = B.assign(B - lmbda*tf.clip_by_value(grad_B, -grad_max, grad_max, name=None))\n",
    "new_U = U.assign(U - learning_rate*grad_U)            \n",
    "new_W = W.assign(W - learning_rate*grad_W)           \n",
    "new_V = V.assign(V - learning_rate*grad_V)          \n",
    "\n",
    "train_step_B = [new_B]\n",
    "train_step = [new_U, new_W, new_V, new_B]\n",
    "\n",
    "all_losses = []\n",
    "all_alignments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Epoch: 200\n",
      "Epoch: 201\n",
      "Epoch: 202\n",
      "Epoch: 203\n",
      "Epoch: 204\n",
      "Epoch: 205\n",
      "Epoch: 206\n",
      "Epoch: 207\n",
      "Epoch: 208\n",
      "Epoch: 209\n",
      "Epoch: 210\n",
      "Epoch: 211\n",
      "Epoch: 212\n",
      "Epoch: 213\n",
      "Epoch: 214\n",
      "Epoch: 215\n",
      "Epoch: 216\n",
      "Epoch: 217\n",
      "Epoch: 218\n",
      "Epoch: 219\n",
      "Epoch: 220\n",
      "Epoch: 221\n",
      "Epoch: 222\n",
      "Epoch: 223\n",
      "Epoch: 224\n",
      "Epoch: 225\n",
      "Epoch: 226\n",
      "Epoch: 227\n",
      "Epoch: 228\n",
      "Epoch: 229\n",
      "Epoch: 230\n",
      "Epoch: 231\n",
      "Epoch: 232\n",
      "Epoch: 233\n",
      "Epoch: 234\n",
      "Epoch: 235\n",
      "Epoch: 236\n",
      "Epoch: 237\n",
      "Epoch: 238\n",
      "Epoch: 239\n",
      "Epoch: 240\n",
      "Epoch: 241\n",
      "Epoch: 242\n",
      "Epoch: 243\n",
      "Epoch: 244\n",
      "Epoch: 245\n",
      "Epoch: 246\n",
      "Epoch: 247\n",
      "Epoch: 248\n",
      "Epoch: 249\n",
      "Epoch: 250\n",
      "Epoch: 251\n",
      "Epoch: 252\n",
      "Epoch: 253\n",
      "Epoch: 254\n",
      "Epoch: 255\n",
      "Epoch: 256\n",
      "Epoch: 257\n",
      "Epoch: 258\n",
      "Epoch: 259\n",
      "Epoch: 260\n",
      "Epoch: 261\n",
      "Epoch: 262\n",
      "Epoch: 263\n",
      "Epoch: 264\n",
      "Epoch: 265\n",
      "Epoch: 266\n",
      "Epoch: 267\n",
      "Epoch: 268\n",
      "Epoch: 269\n",
      "Epoch: 270\n",
      "Epoch: 271\n",
      "Epoch: 272\n",
      "Epoch: 273\n",
      "Epoch: 274\n",
      "Epoch: 275\n",
      "Epoch: 276\n",
      "Epoch: 277\n",
      "Epoch: 278\n",
      "Epoch: 279\n",
      "Epoch: 280\n",
      "Epoch: 281\n",
      "Epoch: 282\n",
      "Epoch: 283\n",
      "Epoch: 284\n",
      "Epoch: 285\n",
      "Epoch: 286\n",
      "Epoch: 287\n",
      "Epoch: 288\n",
      "Epoch: 289\n",
      "Epoch: 290\n",
      "Epoch: 291\n",
      "Epoch: 292\n",
      "Epoch: 293\n",
      "Epoch: 294\n",
      "Epoch: 295\n",
      "Epoch: 296\n",
      "Epoch: 297\n",
      "Epoch: 298\n",
      "Epoch: 299\n",
      "Epoch: 300\n",
      "Epoch: 301\n",
      "Epoch: 302\n",
      "Epoch: 303\n",
      "Epoch: 304\n",
      "Epoch: 305\n",
      "Epoch: 306\n",
      "Epoch: 307\n",
      "Epoch: 308\n",
      "Epoch: 309\n",
      "Epoch: 310\n",
      "Epoch: 311\n",
      "Epoch: 312\n",
      "Epoch: 313\n",
      "Epoch: 314\n",
      "Epoch: 315\n",
      "Epoch: 316\n",
      "Epoch: 317\n",
      "Epoch: 318\n",
      "Epoch: 319\n",
      "Epoch: 320\n",
      "Epoch: 321\n",
      "Epoch: 322\n",
      "Epoch: 323\n",
      "Epoch: 324\n",
      "Epoch: 325\n",
      "Epoch: 326\n",
      "Epoch: 327\n",
      "Epoch: 328\n",
      "Epoch: 329\n",
      "Epoch: 330\n",
      "Epoch: 331\n",
      "Epoch: 332\n",
      "Epoch: 333\n",
      "Epoch: 334\n",
      "Epoch: 335\n",
      "Epoch: 336\n",
      "Epoch: 337\n",
      "Epoch: 338\n",
      "Epoch: 339\n",
      "Epoch: 340\n",
      "Epoch: 341\n",
      "Epoch: 342\n",
      "Epoch: 343\n",
      "Epoch: 344\n",
      "Epoch: 345\n",
      "Epoch: 346\n",
      "Epoch: 347\n",
      "Epoch: 348\n",
      "Epoch: 349\n",
      "Epoch: 350\n",
      "Epoch: 351\n",
      "Epoch: 352\n",
      "Epoch: 353\n",
      "Epoch: 354\n",
      "Epoch: 355\n",
      "Epoch: 356\n",
      "Epoch: 357\n",
      "Epoch: 358\n",
      "Epoch: 359\n",
      "Epoch: 360\n",
      "Epoch: 361\n",
      "Epoch: 362\n",
      "Epoch: 363\n",
      "Epoch: 364\n",
      "Epoch: 365\n",
      "Epoch: 366\n",
      "Epoch: 367\n",
      "Epoch: 368\n",
      "Epoch: 369\n",
      "Epoch: 370\n",
      "Epoch: 371\n",
      "Epoch: 372\n",
      "Epoch: 373\n",
      "Epoch: 374\n",
      "Epoch: 375\n",
      "Epoch: 376\n",
      "Epoch: 377\n",
      "Epoch: 378\n",
      "Epoch: 379\n",
      "Epoch: 380\n",
      "Epoch: 381\n",
      "Epoch: 382\n",
      "Epoch: 383\n",
      "Epoch: 384\n",
      "Epoch: 385\n",
      "Epoch: 386\n",
      "Epoch: 387\n",
      "Epoch: 388\n",
      "Epoch: 389\n",
      "Epoch: 390\n",
      "Epoch: 391\n",
      "Epoch: 392\n",
      "Epoch: 393\n",
      "Epoch: 394\n",
      "Epoch: 395\n",
      "Epoch: 396\n",
      "Epoch: 397\n",
      "Epoch: 398\n",
      "Epoch: 399\n",
      "Epoch: 400\n",
      "Epoch: 401\n",
      "Epoch: 402\n",
      "Epoch: 403\n",
      "Epoch: 404\n",
      "Epoch: 405\n",
      "Epoch: 406\n",
      "Epoch: 407\n",
      "Epoch: 408\n",
      "Epoch: 409\n",
      "Epoch: 410\n",
      "Epoch: 411\n",
      "Epoch: 412\n",
      "Epoch: 413\n",
      "Epoch: 414\n",
      "Epoch: 415\n",
      "Epoch: 416\n",
      "Epoch: 417\n",
      "Epoch: 418\n",
      "Epoch: 419\n",
      "Epoch: 420\n",
      "Epoch: 421\n",
      "Epoch: 422\n",
      "Epoch: 423\n",
      "Epoch: 424\n",
      "Epoch: 425\n",
      "Epoch: 426\n",
      "Epoch: 427\n",
      "Epoch: 428\n",
      "Epoch: 429\n",
      "Epoch: 430\n",
      "Epoch: 431\n",
      "Epoch: 432\n",
      "Epoch: 433\n",
      "Epoch: 434\n",
      "Epoch: 435\n",
      "Epoch: 436\n",
      "Epoch: 437\n",
      "Epoch: 438\n",
      "Epoch: 439\n",
      "Epoch: 440\n",
      "Epoch: 441\n",
      "Epoch: 442\n",
      "Epoch: 443\n",
      "Epoch: 444\n",
      "Epoch: 445\n",
      "Epoch: 446\n",
      "Epoch: 447\n",
      "Epoch: 448\n",
      "Epoch: 449\n",
      "Epoch: 450\n",
      "Epoch: 451\n",
      "Epoch: 452\n",
      "Epoch: 453\n",
      "Epoch: 454\n",
      "Epoch: 455\n",
      "Epoch: 456\n",
      "Epoch: 457\n",
      "Epoch: 458\n",
      "Epoch: 459\n",
      "Epoch: 460\n",
      "Epoch: 461\n",
      "Epoch: 462\n",
      "Epoch: 463\n",
      "Epoch: 464\n",
      "Epoch: 465\n",
      "Epoch: 466\n",
      "Epoch: 467\n",
      "Epoch: 468\n",
      "Epoch: 469\n",
      "Epoch: 470\n",
      "Epoch: 471\n",
      "Epoch: 472\n",
      "Epoch: 473\n",
      "Epoch: 474\n",
      "Epoch: 475\n",
      "Epoch: 476\n",
      "Epoch: 477\n",
      "Epoch: 478\n",
      "Epoch: 479\n",
      "Epoch: 480\n",
      "Epoch: 481\n",
      "Epoch: 482\n",
      "Epoch: 483\n",
      "Epoch: 484\n",
      "Epoch: 485\n",
      "Epoch: 486\n",
      "Epoch: 487\n",
      "Epoch: 488\n",
      "Epoch: 489\n",
      "Epoch: 490\n",
      "Epoch: 491\n",
      "Epoch: 492\n",
      "Epoch: 493\n",
      "Epoch: 494\n",
      "Epoch: 495\n",
      "Epoch: 496\n",
      "Epoch: 497\n",
      "Epoch: 498\n",
      "Epoch: 499\n",
      "Epoch: 500\n",
      "Epoch: 501\n",
      "Epoch: 502\n",
      "Epoch: 503\n",
      "Epoch: 504\n",
      "Epoch: 505\n",
      "Epoch: 506\n",
      "Epoch: 507\n",
      "Epoch: 508\n",
      "Epoch: 509\n",
      "Epoch: 510\n",
      "Epoch: 511\n",
      "Epoch: 512\n",
      "Epoch: 513\n",
      "Epoch: 514\n",
      "Epoch: 515\n",
      "Epoch: 516\n",
      "Epoch: 517\n",
      "Epoch: 518\n",
      "Epoch: 519\n",
      "Epoch: 520\n",
      "Epoch: 521\n",
      "Epoch: 522\n",
      "Epoch: 523\n",
      "Epoch: 524\n",
      "Epoch: 525\n",
      "Epoch: 526\n",
      "Epoch: 527\n",
      "Epoch: 528\n",
      "Epoch: 529\n",
      "Epoch: 530\n",
      "Epoch: 531\n",
      "Epoch: 532\n",
      "Epoch: 533\n",
      "Epoch: 534\n",
      "Epoch: 535\n",
      "Epoch: 536\n",
      "Epoch: 537\n",
      "Epoch: 538\n",
      "Epoch: 539\n",
      "Epoch: 540\n",
      "Epoch: 541\n",
      "Epoch: 542\n",
      "Epoch: 543\n",
      "Epoch: 544\n",
      "Epoch: 545\n",
      "Epoch: 546\n",
      "Epoch: 547\n",
      "Epoch: 548\n",
      "Epoch: 549\n",
      "Epoch: 550\n",
      "Epoch: 551\n",
      "Epoch: 552\n",
      "Epoch: 553\n",
      "Epoch: 554\n",
      "Epoch: 555\n",
      "Epoch: 556\n",
      "Epoch: 557\n",
      "Epoch: 558\n",
      "Epoch: 559\n",
      "Epoch: 560\n",
      "Epoch: 561\n",
      "Epoch: 562\n",
      "Epoch: 563\n",
      "Epoch: 564\n",
      "Epoch: 565\n",
      "Epoch: 566\n",
      "Epoch: 567\n",
      "Epoch: 568\n",
      "Epoch: 569\n",
      "Epoch: 570\n",
      "Epoch: 571\n",
      "Epoch: 572\n",
      "Epoch: 573\n",
      "Epoch: 574\n",
      "Epoch: 575\n",
      "Epoch: 576\n",
      "Epoch: 577\n",
      "Epoch: 578\n",
      "Epoch: 579\n",
      "Epoch: 580\n",
      "Epoch: 581\n",
      "Epoch: 582\n",
      "Epoch: 583\n",
      "Epoch: 584\n",
      "Epoch: 585\n",
      "Epoch: 586\n",
      "Epoch: 587\n",
      "Epoch: 588\n",
      "Epoch: 589\n",
      "Epoch: 590\n",
      "Epoch: 591\n",
      "Epoch: 592\n",
      "Epoch: 593\n",
      "Epoch: 594\n",
      "Epoch: 595\n",
      "Epoch: 596\n",
      "Epoch: 597\n",
      "Epoch: 598\n",
      "Epoch: 599\n",
      "Epoch: 600\n",
      "Epoch: 601\n",
      "Epoch: 602\n",
      "Epoch: 603\n",
      "Epoch: 604\n",
      "Epoch: 605\n",
      "Epoch: 606\n",
      "Epoch: 607\n",
      "Epoch: 608\n",
      "Epoch: 609\n",
      "Epoch: 610\n",
      "Epoch: 611\n",
      "Epoch: 612\n",
      "Epoch: 613\n",
      "Epoch: 614\n",
      "Epoch: 615\n",
      "Epoch: 616\n",
      "Epoch: 617\n",
      "Epoch: 618\n",
      "Epoch: 619\n",
      "Epoch: 620\n",
      "Epoch: 621\n",
      "Epoch: 622\n",
      "Epoch: 623\n",
      "Epoch: 624\n",
      "Epoch: 625\n",
      "Epoch: 626\n",
      "Epoch: 627\n",
      "Epoch: 628\n",
      "Epoch: 629\n",
      "Epoch: 630\n",
      "Epoch: 631\n",
      "Epoch: 632\n",
      "Epoch: 633\n",
      "Epoch: 634\n",
      "Epoch: 635\n",
      "Epoch: 636\n",
      "Epoch: 637\n",
      "Epoch: 638\n",
      "Epoch: 639\n",
      "Epoch: 640\n",
      "Epoch: 641\n",
      "Epoch: 642\n",
      "Epoch: 643\n",
      "Epoch: 644\n",
      "Epoch: 645\n",
      "Epoch: 646\n",
      "Epoch: 647\n",
      "Epoch: 648\n",
      "Epoch: 649\n",
      "Epoch: 650\n",
      "Epoch: 651\n",
      "Epoch: 652\n",
      "Epoch: 653\n",
      "Epoch: 654\n",
      "Epoch: 655\n",
      "Epoch: 656\n",
      "Epoch: 657\n",
      "Epoch: 658\n",
      "Epoch: 659\n",
      "Epoch: 660\n",
      "Epoch: 661\n",
      "Epoch: 662\n",
      "Epoch: 663\n",
      "Epoch: 664\n",
      "Epoch: 665\n",
      "Epoch: 666\n",
      "Epoch: 667\n",
      "Epoch: 668\n",
      "Epoch: 669\n",
      "Epoch: 670\n",
      "Epoch: 671\n",
      "Epoch: 672\n",
      "Epoch: 673\n",
      "Epoch: 674\n",
      "Epoch: 675\n",
      "Epoch: 676\n",
      "Epoch: 677\n",
      "Epoch: 678\n",
      "Epoch: 679\n",
      "Epoch: 680\n",
      "Epoch: 681\n",
      "Epoch: 682\n",
      "Epoch: 683\n",
      "Epoch: 684\n",
      "Epoch: 685\n",
      "Epoch: 686\n",
      "Epoch: 687\n",
      "Epoch: 688\n",
      "Epoch: 689\n",
      "Epoch: 690\n",
      "Epoch: 691\n",
      "Epoch: 692\n",
      "Epoch: 693\n",
      "Epoch: 694\n",
      "Epoch: 695\n",
      "Epoch: 696\n",
      "Epoch: 697\n",
      "Epoch: 698\n",
      "Epoch: 699\n",
      "Epoch: 700\n",
      "Epoch: 701\n",
      "Epoch: 702\n",
      "Epoch: 703\n",
      "Epoch: 704\n",
      "Epoch: 705\n",
      "Epoch: 706\n",
      "Epoch: 707\n",
      "Epoch: 708\n",
      "Epoch: 709\n",
      "Epoch: 710\n",
      "Epoch: 711\n",
      "Epoch: 712\n",
      "Epoch: 713\n",
      "Epoch: 714\n",
      "Epoch: 715\n",
      "Epoch: 716\n",
      "Epoch: 717\n",
      "Epoch: 718\n",
      "Epoch: 719\n",
      "Epoch: 720\n",
      "Epoch: 721\n",
      "Epoch: 722\n",
      "Epoch: 723\n",
      "Epoch: 724\n",
      "Epoch: 725\n",
      "Epoch: 726\n",
      "Epoch: 727\n",
      "Epoch: 728\n",
      "Epoch: 729\n",
      "Epoch: 730\n",
      "Epoch: 731\n",
      "Epoch: 732\n",
      "Epoch: 733\n",
      "Epoch: 734\n",
      "Epoch: 735\n",
      "Epoch: 736\n",
      "Epoch: 737\n",
      "Epoch: 738\n",
      "Epoch: 739\n",
      "Epoch: 740\n",
      "Epoch: 741\n",
      "Epoch: 742\n",
      "Epoch: 743\n",
      "Epoch: 744\n",
      "Epoch: 745\n",
      "Epoch: 746\n",
      "Epoch: 747\n",
      "Epoch: 748\n",
      "Epoch: 749\n",
      "Epoch: 750\n",
      "Epoch: 751\n",
      "Epoch: 752\n",
      "Epoch: 753\n",
      "Epoch: 754\n",
      "Epoch: 755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 756\n",
      "Epoch: 757\n",
      "Epoch: 758\n",
      "Epoch: 759\n",
      "Epoch: 760\n",
      "Epoch: 761\n",
      "Epoch: 762\n",
      "Epoch: 763\n",
      "Epoch: 764\n",
      "Epoch: 765\n",
      "Epoch: 766\n",
      "Epoch: 767\n",
      "Epoch: 768\n",
      "Epoch: 769\n",
      "Epoch: 770\n",
      "Epoch: 771\n",
      "Epoch: 772\n",
      "Epoch: 773\n",
      "Epoch: 774\n",
      "Epoch: 775\n",
      "Epoch: 776\n",
      "Epoch: 777\n",
      "Epoch: 778\n",
      "Epoch: 779\n",
      "Epoch: 780\n",
      "Epoch: 781\n",
      "Epoch: 782\n",
      "Epoch: 783\n",
      "Epoch: 784\n",
      "Epoch: 785\n",
      "Epoch: 786\n",
      "Epoch: 787\n",
      "Epoch: 788\n",
      "Epoch: 789\n",
      "Epoch: 790\n",
      "Epoch: 791\n",
      "Epoch: 792\n",
      "Epoch: 793\n",
      "Epoch: 794\n",
      "Epoch: 795\n",
      "Epoch: 796\n",
      "Epoch: 797\n",
      "Epoch: 798\n",
      "Epoch: 799\n",
      "Epoch: 800\n",
      "Epoch: 801\n",
      "Epoch: 802\n",
      "Epoch: 803\n",
      "Epoch: 804\n",
      "Epoch: 805\n",
      "Epoch: 806\n",
      "Epoch: 807\n",
      "Epoch: 808\n",
      "Epoch: 809\n",
      "Epoch: 810\n",
      "Epoch: 811\n",
      "Epoch: 812\n",
      "Epoch: 813\n",
      "Epoch: 814\n",
      "Epoch: 815\n",
      "Epoch: 816\n",
      "Epoch: 817\n",
      "Epoch: 818\n",
      "Epoch: 819\n",
      "Epoch: 820\n",
      "Epoch: 821\n",
      "Epoch: 822\n",
      "Epoch: 823\n",
      "Epoch: 824\n",
      "Epoch: 825\n",
      "Epoch: 826\n",
      "Epoch: 827\n",
      "Epoch: 828\n",
      "Epoch: 829\n",
      "Epoch: 830\n",
      "Epoch: 831\n",
      "Epoch: 832\n",
      "Epoch: 833\n",
      "Epoch: 834\n",
      "Epoch: 835\n",
      "Epoch: 836\n",
      "Epoch: 837\n",
      "Epoch: 838\n",
      "Epoch: 839\n",
      "Epoch: 840\n",
      "Epoch: 841\n",
      "Epoch: 842\n",
      "Epoch: 843\n",
      "Epoch: 844\n",
      "Epoch: 845\n",
      "Epoch: 846\n",
      "Epoch: 847\n",
      "Epoch: 848\n",
      "Epoch: 849\n",
      "Epoch: 850\n",
      "Epoch: 851\n",
      "Epoch: 852\n",
      "Epoch: 853\n",
      "Epoch: 854\n",
      "Epoch: 855\n",
      "Epoch: 856\n",
      "Epoch: 857\n",
      "Epoch: 858\n",
      "Epoch: 859\n",
      "Epoch: 860\n",
      "Epoch: 861\n",
      "Epoch: 862\n",
      "Epoch: 863\n",
      "Epoch: 864\n",
      "Epoch: 865\n",
      "Epoch: 866\n",
      "Epoch: 867\n",
      "Epoch: 868\n",
      "Epoch: 869\n",
      "Epoch: 870\n",
      "Epoch: 871\n",
      "Epoch: 872\n",
      "Epoch: 873\n",
      "Epoch: 874\n",
      "Epoch: 875\n",
      "Epoch: 876\n",
      "Epoch: 877\n",
      "Epoch: 878\n",
      "Epoch: 879\n",
      "Epoch: 880\n",
      "Epoch: 881\n",
      "Epoch: 882\n",
      "Epoch: 883\n",
      "Epoch: 884\n",
      "Epoch: 885\n",
      "Epoch: 886\n",
      "Epoch: 887\n",
      "Epoch: 888\n",
      "Epoch: 889\n",
      "Epoch: 890\n",
      "Epoch: 891\n",
      "Epoch: 892\n",
      "Epoch: 893\n",
      "Epoch: 894\n",
      "Epoch: 895\n",
      "Epoch: 896\n",
      "Epoch: 897\n",
      "Epoch: 898\n",
      "Epoch: 899\n",
      "Epoch: 900\n",
      "Epoch: 901\n",
      "Epoch: 902\n",
      "Epoch: 903\n",
      "Epoch: 904\n",
      "Epoch: 905\n",
      "Epoch: 906\n",
      "Epoch: 907\n",
      "Epoch: 908\n",
      "Epoch: 909\n",
      "Epoch: 910\n",
      "Epoch: 911\n",
      "Epoch: 912\n",
      "Epoch: 913\n",
      "Epoch: 914\n",
      "Epoch: 915\n",
      "Epoch: 916\n",
      "Epoch: 917\n",
      "Epoch: 918\n",
      "Epoch: 919\n",
      "Epoch: 920\n",
      "Epoch: 921\n",
      "Epoch: 922\n",
      "Epoch: 923\n",
      "Epoch: 924\n",
      "Epoch: 925\n",
      "Epoch: 926\n",
      "Epoch: 927\n",
      "Epoch: 928\n",
      "Epoch: 929\n",
      "Epoch: 930\n",
      "Epoch: 931\n",
      "Epoch: 932\n",
      "Epoch: 933\n",
      "Epoch: 934\n",
      "Epoch: 935\n",
      "Epoch: 936\n",
      "Epoch: 937\n",
      "Epoch: 938\n",
      "Epoch: 939\n",
      "Epoch: 940\n",
      "Epoch: 941\n",
      "Epoch: 942\n",
      "Epoch: 943\n",
      "Epoch: 944\n",
      "Epoch: 945\n",
      "Epoch: 946\n",
      "Epoch: 947\n",
      "Epoch: 948\n",
      "Epoch: 949\n",
      "Epoch: 950\n",
      "Epoch: 951\n",
      "Epoch: 952\n",
      "Epoch: 953\n",
      "Epoch: 954\n",
      "Epoch: 955\n",
      "Epoch: 956\n",
      "Epoch: 957\n",
      "Epoch: 958\n",
      "Epoch: 959\n",
      "Epoch: 960\n",
      "Epoch: 961\n",
      "Epoch: 962\n",
      "Epoch: 963\n",
      "Epoch: 964\n",
      "Epoch: 965\n",
      "Epoch: 966\n",
      "Epoch: 967\n",
      "Epoch: 968\n",
      "Epoch: 969\n",
      "Epoch: 970\n",
      "Epoch: 971\n",
      "Epoch: 972\n",
      "Epoch: 973\n",
      "Epoch: 974\n",
      "Epoch: 975\n",
      "Epoch: 976\n",
      "Epoch: 977\n",
      "Epoch: 978\n",
      "Epoch: 979\n",
      "Epoch: 980\n",
      "Epoch: 981\n",
      "Epoch: 982\n",
      "Epoch: 983\n",
      "Epoch: 984\n",
      "Epoch: 985\n",
      "Epoch: 986\n",
      "Epoch: 987\n",
      "Epoch: 988\n",
      "Epoch: 989\n",
      "Epoch: 990\n",
      "Epoch: 991\n",
      "Epoch: 992\n",
      "Epoch: 993\n",
      "Epoch: 994\n",
      "Epoch: 995\n",
      "Epoch: 996\n",
      "Epoch: 997\n",
      "Epoch: 998\n",
      "Epoch: 999\n"
     ]
    }
   ],
   "source": [
    "training_losses, n_in_epoch, alignments, xs = train_network(N_episodes, num_steps)\n",
    "all_losses.append(training_losses)\n",
    "all_alignments.append(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make some plots to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 10, 20, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = 5\n",
    "nB = 20\n",
    "theta_d = xs[:,:,:,:,0].reshape(N_epochs,-1, nB)\n",
    "theta = xs[:,:,:,:,1].reshape(N_epochs,-1,nB)\n",
    "h_d = xs[:,:,:,:,2].reshape(N_epochs,-1,nB)\n",
    "h = xs[:,:,:,:,3].reshape(N_epochs,-1,nB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853981633974483"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda2/envs/py34/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAFJCAYAAAAxJF5AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axis = plt.subplots(1,1,figsize = (5,5))\n",
    "#axis.plot(np.arange(5000),np.mean(theta<np.pi/8, 1))\n",
    "sns.tsplot(np.mean(theta<np.pi/8, 1).T, ax = axis, ci=68)\n",
    "axis.set_xlabel('trial #')\n",
    "axis.set_xlim([0, 5000])\n",
    "axis.set_ylabel('proportion of time upright')\n",
    "sns.despine(trim=True)\n",
    "#plt.savefig('./figues/fig_3_cartpole_bp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
