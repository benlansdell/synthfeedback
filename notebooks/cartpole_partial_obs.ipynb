{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lansdell/projects/synthfeedback\n"
     ]
    }
   ],
   "source": [
    "cd /home/lansdell/projects/synthfeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from numpy import random as rng\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole RNN with partial observability.\n",
    "\n",
    "Only observe the position and angle -- requires integration over time to solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backprop\n",
      "For 1000 epochs\n",
      "Learning rate: 0.001000\n",
      "Lambda learning rate: 0.000050\n",
      "Variance xi: 0.500000\n",
      "Saving results: 0\n"
     ]
    }
   ],
   "source": [
    "#method = args.method\n",
    "#save = bool(args.save)\n",
    "\n",
    "method = 'backprop'\n",
    "save = False\n",
    "\n",
    "anneal = True\n",
    "\n",
    "# Global config variables\n",
    "num_steps = 10 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 20\n",
    "in_dim = 4\n",
    "state_size = 50\n",
    "learning_rate = 1e-3\n",
    "alpha2 = 1\n",
    "activation = tf.tanh\n",
    "act_prime = lambda x: 1 - tf.multiply(x,x)\n",
    "\n",
    "#Node pert params\n",
    "lmbda = 5e-5\n",
    "var_xi = 0.5\n",
    "p_fire = 0.1 #prob of firing\n",
    "\n",
    "acclimatize = True\n",
    "\n",
    "grad_max = 10\n",
    "\n",
    "N_epochs = 1000\n",
    "N_episodes = 10\n",
    "\n",
    "report_rate = 100\n",
    "fn_out = './experiments/cartpole_rnn_np/%s_learning_rate_%f_lmbda_%f_varxi_%f.npz'%(method, learning_rate, lmbda, var_xi)\n",
    "\n",
    "#Things to save with output\n",
    "params = {\n",
    "'num_steps': num_steps,\n",
    "'batch_size': batch_size,\n",
    "'in_dim': in_dim,\n",
    "'state_size': state_size,\n",
    "'learning_rate': learning_rate,\n",
    "'alpha2': alpha2,\n",
    "'lmbda': lmbda,\n",
    "'var_xi': var_xi,\n",
    "'p_fire': p_fire,\n",
    "'grad_max': grad_max,\n",
    "'N_epochs': N_epochs,\n",
    "'N_episodes': N_episodes,\n",
    "'acclimatize':acclimatize\n",
    "}\n",
    "\n",
    "#method = 'backprop'\n",
    "#method = 'feedbackalignment'\n",
    "#method = 'nodepert'\n",
    "#method = 'weightsym'\n",
    "\n",
    "print(\"Using %s\"%method)\n",
    "print(\"For %d epochs\"%N_epochs)\n",
    "print(\"Learning rate: %f\"%learning_rate)\n",
    "print(\"Lambda learning rate: %f\"%lmbda)\n",
    "print(\"Variance xi: %f\"%var_xi)\n",
    "print(\"Saving results: %d\"%save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.15685517e+00 -1.29428169e+00  4.51830099e-01 -1.00536273e+00\n",
      "   7.71773708e-01]\n",
      " [-4.04075149e-01  1.63761849e+00 -9.65823496e-01 -2.73188162e-01\n",
      "  -1.46620817e+00]\n",
      " [ 3.14933547e-01  1.84941873e+00  5.88634090e-01 -1.24419116e+00\n",
      "   2.32919284e-01]\n",
      " [ 3.12487378e-01  1.30745682e+00  2.42652460e+00 -2.30228933e-03\n",
      "  -7.75277827e-01]]\n",
      "[[-1.15685517  0.4518301 ]\n",
      " [-0.40407515 -0.9658235 ]\n",
      " [ 0.31493355  0.58863409]\n",
      " [ 0.31248738  2.4265246 ]]\n"
     ]
    }
   ],
   "source": [
    "ar = rng.randn(4,5)\n",
    "print(ar)\n",
    "print(ar[:,0:-1:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell(rnn_input, state, W, U, B):\n",
    "    ones0 = tf.ones([batch_size, 1], tf.float32)\n",
    "    state_p = tf.concat([state, ones0], 1)\n",
    "    return activation(tf.matmul(rnn_input[:,0:-1:2], U) + tf.matmul(state_p, W))\n",
    "    #return activation(tf_matmul_r(rnn_input[:,0:-1:2], U, B[0:2,:]) + tf_matmul_r(state_p, W, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_episodes, num_steps, state_size=state_size, verbose=True):\n",
    "    xs = np.zeros((N_epochs, num_episodes, num_steps, batch_size, in_dim))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        alignments = []\n",
    "\n",
    "        for idx in range(N_epochs):\n",
    "            print(\"Epoch: %d\"%idx)\n",
    "            #if idx < 4 and acclimatize:\n",
    "            #    ts = train_step_B\n",
    "            #else:\n",
    "            ts = train_step\n",
    "            training_loss = 0\n",
    "            training_x = np.zeros((batch_size, in_dim))\n",
    "            #training_x = 2*rng.randn(batch_size, in_dim)\n",
    "            #training_x[:,1] = np.pi\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            for step in range(num_episodes):\n",
    "                tr_init_gradW = np.zeros((state_size+1, state_size))\n",
    "                tr_init_gradB = np.zeros((state_size+1, state_size))\n",
    "                tr_init_gradU = np.zeros((int(in_dim/2), state_size))\n",
    "                tr_loss, tr_losses, training_loss_, training_state, training_x, _, align, x_o = \\\n",
    "                    sess.run([loss, losses, total_loss, final_state, final_x, ts, aments, rnn_inputs], \\\n",
    "                                  feed_dict={init_state:training_state, init_x: training_x, \\\n",
    "                                  init_gradU: tr_init_gradU, init_gradW: tr_init_gradW, \\\n",
    "                                  init_gradB: tr_init_gradB})\n",
    "                #print(np.array(x_o).shape)\n",
    "                xs[idx, step, :, :, :] = np.array(x_o)[:,:,:]\n",
    "                training_loss += training_loss_\n",
    "                if step % report_rate == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step %d for last %d steps: %f\"%(step, report_rate, \\\n",
    "                                                                               training_loss/report_rate))\n",
    "                    training_losses.append(training_loss/report_rate)\n",
    "                    alignments.append(align)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses, step, alignments, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "## BACKPROP ##\n",
    "##############\n",
    "\n",
    "def backprop():\n",
    "\n",
    "    grad_U = init_gradU\n",
    "    grad_W = init_gradW\n",
    "    grad_B = init_gradB\n",
    "    alnments = []\n",
    "#     for i in range(num_steps):\n",
    "#         for j in range(i+1)[::-1]:\n",
    "#             if j == i:\n",
    "#                 delta = tf.multiply(tf.matmul(delta0s[i][:,None],tf.transpose(V[0:state_size,:])), \\\n",
    "#                                     act_prime(rnn_outputs[j]))\n",
    "#             else:\n",
    "#                 delta = tf.multiply(tf.matmul(delta, tf.transpose(W[0:state_size,:])), act_prime(rnn_outputs[j]))\n",
    "#             grad_U = grad_U + tf.matmul(tf.transpose(rnn_inputs[j]), delta)\n",
    "#             if j > 0:\n",
    "#                 grad_W = grad_W + tf.matmul(tf.transpose(tf.concat([rnn_outputs[j-1], ones0],1)), delta)\n",
    "\n",
    "    grad_V = tf.gradients(xs=V, ys=total_loss)[0]\n",
    "    grad_W = tf.gradients(xs=W, ys=total_loss)[0]\n",
    "    grad_U = tf.gradients(xs=U, ys=total_loss)[0]\n",
    "\n",
    "    return grad_U, grad_W, grad_B, grad_V, alnments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if method == 'backprop':\n",
    "    trainer = backprop\n",
    "elif method == 'feedbackalignment':\n",
    "    trainer = feedbackalignment\n",
    "elif method == 'nodepert':\n",
    "    trainer = nodepert\n",
    "elif method == 'weightsym':\n",
    "    trainer = weightsym\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "init_x = tf.zeros([batch_size, in_dim], dtype=np.float32)\n",
    "init_state = tf.zeros([batch_size, state_size], dtype=np.float32)\n",
    "init_gradW = tf.zeros([state_size+1, state_size], dtype=np.float32)\n",
    "init_gradB = tf.zeros([state_size+1, state_size], dtype=np.float32)\n",
    "init_gradU = tf.zeros([in_dim/2, state_size], dtype=np.float32)\n",
    "alignment = tf.zeros([in_dim/2, state_size], dtype=np.float32)\n",
    "\n",
    "ones0 = tf.ones([batch_size, 1], tf.float32)\n",
    "#U = tf.get_variable('U', [in_dim, state_size])\n",
    "#W = tf.get_variable('W', [state_size+1, state_size])\n",
    "#V = tf.get_variable('V', [state_size+1, 1])\n",
    "U = tf.Variable(rng.randn(int(in_dim/2), state_size)*alpha2, name=\"input_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(state_size+1, state_size)*alpha2, name=\"feedforward_weights\", dtype=tf.float32)\n",
    "#W = tf.Variable(np.zeros((state_size+1, state_size))*alpha2, name=\"feedforward_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(state_size+1, state_size)*alpha2, name=\"feedback_weights\", dtype=tf.float32)\n",
    "#V = tf.Variable(rng.randn(state_size+1, 1)*alpha2, name=\"output_weights\", dtype=tf.float32)\n",
    "V = tf.Variable(rng.randn(state_size+1, 1)*alpha2, name=\"output_weights\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pole task and dynamics here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = init_x\n",
    "state = init_state\n",
    "state_p = init_state\n",
    "rnn_inputs = []\n",
    "rnn_outputs = []\n",
    "rnn_pert_outputs = []\n",
    "noise_outputs = []\n",
    "heights = []\n",
    "hs = []\n",
    "actions = []\n",
    "\n",
    "#Cart pole dynamics simulated for num_steps \n",
    "m = 1.1\n",
    "mp = 0.1\n",
    "g = 9.8\n",
    "l = 0.5\n",
    "tau = 0.04\n",
    "Fmax = 10\n",
    "max_h = 3\n",
    "gamma = 10\n",
    "\n",
    "#Equations of motion:\n",
    "#theta_dd = (m*g*sin(theta) - cos(theta)*(F + mp*l*theta_d*theta_d*sin(theta)))/((4/3)*m*l - mp*l*cos(theta)*cos(theta))\n",
    "#theta_d += tau*theta_dd\n",
    "#theta += tau*theta\n",
    "#h_dd = (F + mp*l*(theta_d*theta_d*sin(theta)-theta_dd*cos(theta)))/m\n",
    "#h_d += tau*h_dd\n",
    "#h += tau*h_d\n",
    "\n",
    "for idx in range(num_steps):\n",
    "    mask = tf.random_uniform([batch_size, state_size]) < p_fire\n",
    "    xi = tf.multiply(tf.random_normal([batch_size, state_size])*var_xi, tf.to_float(mask))\n",
    "    phi = tf.random_normal((batch_size,1))*Fmax/5\n",
    "    #Compute new state\n",
    "    state = rnn_cell(x, state, W, U, B)\n",
    "    state_p = rnn_cell(x, state_p, W, U, B) + xi\n",
    "    #Compute action\n",
    "    action = tf.matmul(tf.concat([state, ones0], 1), V)\n",
    "    F = tf.squeeze(Fmax*activation(action) + phi)\n",
    "    #Compute new x\n",
    "    theta_dd = (m*g*tf.sin(x[:,1]) - tf.cos(x[:,1])*(F + mp*l*x[:,0]*x[:,0]*tf.sin(x[:,1])))/((4/3)*m*l -\\\n",
    "                mp*l*tf.cos(x[:,1])*tf.cos(x[:,1]))\n",
    "    h_dd = (F + mp*l*(x[:,0]*x[:,0]*tf.sin(x[:,1])-theta_dd*tf.cos(x[:,1])))/m\n",
    "    \n",
    "    #h_dd = (F - mp*l*x[:,0]*x[:,0]*tf.sin(x[:,1]) + mp*g*tf.sin(x[:,1])*tf.cos(x[:,1]))/(m - mp*tf.cos(x[:,1])*tf.cos(x[:,1]))\n",
    "    #theta_dd = (h_dd*tf.cos(x[:,1]) + g*tf.sin(x[:,1]))/l \n",
    "\n",
    "    x_list = []\n",
    "    x_list.append(x[:,0] + tau*theta_dd)   #x0 = theta_dot\n",
    "    x_list.append(x[:,1] + tau*x[:,0])     #x1 = theta\n",
    "    x_list.append(x[:,2] + tau*h_dd)       #x2 = h_dot\n",
    "    x_list.append(x[:,3] + tau*x[:,2])     #x3 = h\n",
    "    #x_list.append(tf.clip_by_value(x[:,3] + tau*x[:,2], -4*max_h, 4*max_h))     #x3 = h\n",
    "    x = tf.stack(x_list, axis = 1)\n",
    "    #height = tf.cos(x[:,1])\n",
    "    height = x[:,1]\n",
    "    heights.append(height)\n",
    "    hs.append(x[:,2])\n",
    "    rnn_inputs.append(x)\n",
    "    rnn_outputs.append(state)\n",
    "    rnn_pert_outputs.append(state_p)\n",
    "    noise_outputs.append(xi)\n",
    "    actions.append(action)\n",
    "    \n",
    "final_x = rnn_inputs[-1]\n",
    "final_state = rnn_outputs[-1]\n",
    "\n",
    "#Define loss function....\n",
    "#loss = \n",
    "\n",
    "loss = [gamma*tf.pow(height, 2)/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "losses = [gamma*tf.reduce_sum(tf.pow(height, 2))/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "#Perturbed outputs and loss\n",
    "loss_pert = [gamma*tf.pow(height, 2)/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "losses_pert = [gamma*tf.reduce_sum(tf.pow(height, 2))/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "total_loss_pert = tf.reduce_mean(losses_pert)\n",
    "\n",
    "#e0s = [(height+1) for height in heights]\n",
    "#delta0s = e0s\n",
    "\n",
    "e0s = [tf.gradients(xs=action, ys=lo)[0][:,0] for (action,lo) in zip(actions, losses)]\n",
    "delta0s = e0s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grad_U, grad_W, grad_B, grad_V, aments = trainer()\n",
    "\n",
    "new_B = B.assign(B - lmbda*tf.clip_by_value(grad_B, -grad_max, grad_max, name=None))\n",
    "new_U = U.assign(U - learning_rate*grad_U)            \n",
    "new_W = W.assign(W - learning_rate*grad_W)           \n",
    "new_V = V.assign(V - learning_rate*grad_V)          \n",
    "\n",
    "train_step_B = [new_B]\n",
    "train_step = [new_U, new_W, new_V, new_B]\n",
    "\n",
    "all_losses = []\n",
    "all_alignments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Epoch: 200\n",
      "Epoch: 201\n",
      "Epoch: 202\n",
      "Epoch: 203\n",
      "Epoch: 204\n",
      "Epoch: 205\n",
      "Epoch: 206\n",
      "Epoch: 207\n",
      "Epoch: 208\n",
      "Epoch: 209\n",
      "Epoch: 210\n",
      "Epoch: 211\n",
      "Epoch: 212\n",
      "Epoch: 213\n",
      "Epoch: 214\n",
      "Epoch: 215\n",
      "Epoch: 216\n",
      "Epoch: 217\n",
      "Epoch: 218\n",
      "Epoch: 219\n",
      "Epoch: 220\n",
      "Epoch: 221\n",
      "Epoch: 222\n",
      "Epoch: 223\n",
      "Epoch: 224\n",
      "Epoch: 225\n",
      "Epoch: 226\n",
      "Epoch: 227\n",
      "Epoch: 228\n",
      "Epoch: 229\n",
      "Epoch: 230\n",
      "Epoch: 231\n",
      "Epoch: 232\n",
      "Epoch: 233\n",
      "Epoch: 234\n",
      "Epoch: 235\n",
      "Epoch: 236\n",
      "Epoch: 237\n",
      "Epoch: 238\n",
      "Epoch: 239\n",
      "Epoch: 240\n",
      "Epoch: 241\n",
      "Epoch: 242\n",
      "Epoch: 243\n",
      "Epoch: 244\n",
      "Epoch: 245\n",
      "Epoch: 246\n",
      "Epoch: 247\n",
      "Epoch: 248\n",
      "Epoch: 249\n",
      "Epoch: 250\n",
      "Epoch: 251\n",
      "Epoch: 252\n",
      "Epoch: 253\n",
      "Epoch: 254\n",
      "Epoch: 255\n",
      "Epoch: 256\n",
      "Epoch: 257\n",
      "Epoch: 258\n",
      "Epoch: 259\n",
      "Epoch: 260\n",
      "Epoch: 261\n",
      "Epoch: 262\n",
      "Epoch: 263\n",
      "Epoch: 264\n",
      "Epoch: 265\n",
      "Epoch: 266\n",
      "Epoch: 267\n",
      "Epoch: 268\n",
      "Epoch: 269\n",
      "Epoch: 270\n",
      "Epoch: 271\n",
      "Epoch: 272\n",
      "Epoch: 273\n",
      "Epoch: 274\n",
      "Epoch: 275\n",
      "Epoch: 276\n",
      "Epoch: 277\n",
      "Epoch: 278\n",
      "Epoch: 279\n",
      "Epoch: 280\n",
      "Epoch: 281\n",
      "Epoch: 282\n",
      "Epoch: 283\n",
      "Epoch: 284\n",
      "Epoch: 285\n",
      "Epoch: 286\n",
      "Epoch: 287\n",
      "Epoch: 288\n",
      "Epoch: 289\n",
      "Epoch: 290\n",
      "Epoch: 291\n",
      "Epoch: 292\n",
      "Epoch: 293\n",
      "Epoch: 294\n",
      "Epoch: 295\n",
      "Epoch: 296\n",
      "Epoch: 297\n",
      "Epoch: 298\n",
      "Epoch: 299\n",
      "Epoch: 300\n",
      "Epoch: 301\n",
      "Epoch: 302\n",
      "Epoch: 303\n",
      "Epoch: 304\n",
      "Epoch: 305\n",
      "Epoch: 306\n",
      "Epoch: 307\n",
      "Epoch: 308\n",
      "Epoch: 309\n",
      "Epoch: 310\n",
      "Epoch: 311\n",
      "Epoch: 312\n",
      "Epoch: 313\n",
      "Epoch: 314\n",
      "Epoch: 315\n",
      "Epoch: 316\n",
      "Epoch: 317\n",
      "Epoch: 318\n",
      "Epoch: 319\n",
      "Epoch: 320\n",
      "Epoch: 321\n",
      "Epoch: 322\n",
      "Epoch: 323\n",
      "Epoch: 324\n",
      "Epoch: 325\n",
      "Epoch: 326\n",
      "Epoch: 327\n",
      "Epoch: 328\n",
      "Epoch: 329\n",
      "Epoch: 330\n",
      "Epoch: 331\n",
      "Epoch: 332\n",
      "Epoch: 333\n",
      "Epoch: 334\n",
      "Epoch: 335\n",
      "Epoch: 336\n",
      "Epoch: 337\n",
      "Epoch: 338\n",
      "Epoch: 339\n",
      "Epoch: 340\n",
      "Epoch: 341\n",
      "Epoch: 342\n",
      "Epoch: 343\n",
      "Epoch: 344\n",
      "Epoch: 345\n",
      "Epoch: 346\n",
      "Epoch: 347\n",
      "Epoch: 348\n",
      "Epoch: 349\n",
      "Epoch: 350\n",
      "Epoch: 351\n",
      "Epoch: 352\n",
      "Epoch: 353\n",
      "Epoch: 354\n",
      "Epoch: 355\n",
      "Epoch: 356\n",
      "Epoch: 357\n",
      "Epoch: 358\n",
      "Epoch: 359\n",
      "Epoch: 360\n",
      "Epoch: 361\n",
      "Epoch: 362\n",
      "Epoch: 363\n",
      "Epoch: 364\n",
      "Epoch: 365\n",
      "Epoch: 366\n",
      "Epoch: 367\n",
      "Epoch: 368\n",
      "Epoch: 369\n",
      "Epoch: 370\n",
      "Epoch: 371\n",
      "Epoch: 372\n",
      "Epoch: 373\n",
      "Epoch: 374\n",
      "Epoch: 375\n",
      "Epoch: 376\n",
      "Epoch: 377\n",
      "Epoch: 378\n",
      "Epoch: 379\n",
      "Epoch: 380\n",
      "Epoch: 381\n",
      "Epoch: 382\n",
      "Epoch: 383\n",
      "Epoch: 384\n",
      "Epoch: 385\n",
      "Epoch: 386\n",
      "Epoch: 387\n",
      "Epoch: 388\n",
      "Epoch: 389\n",
      "Epoch: 390\n",
      "Epoch: 391\n",
      "Epoch: 392\n",
      "Epoch: 393\n",
      "Epoch: 394\n",
      "Epoch: 395\n",
      "Epoch: 396\n",
      "Epoch: 397\n",
      "Epoch: 398\n",
      "Epoch: 399\n",
      "Epoch: 400\n",
      "Epoch: 401\n",
      "Epoch: 402\n",
      "Epoch: 403\n",
      "Epoch: 404\n",
      "Epoch: 405\n",
      "Epoch: 406\n",
      "Epoch: 407\n",
      "Epoch: 408\n",
      "Epoch: 409\n",
      "Epoch: 410\n",
      "Epoch: 411\n",
      "Epoch: 412\n",
      "Epoch: 413\n",
      "Epoch: 414\n",
      "Epoch: 415\n",
      "Epoch: 416\n",
      "Epoch: 417\n",
      "Epoch: 418\n",
      "Epoch: 419\n",
      "Epoch: 420\n",
      "Epoch: 421\n",
      "Epoch: 422\n",
      "Epoch: 423\n",
      "Epoch: 424\n",
      "Epoch: 425\n",
      "Epoch: 426\n",
      "Epoch: 427\n",
      "Epoch: 428\n",
      "Epoch: 429\n",
      "Epoch: 430\n",
      "Epoch: 431\n",
      "Epoch: 432\n",
      "Epoch: 433\n",
      "Epoch: 434\n",
      "Epoch: 435\n",
      "Epoch: 436\n",
      "Epoch: 437\n",
      "Epoch: 438\n",
      "Epoch: 439\n",
      "Epoch: 440\n",
      "Epoch: 441\n",
      "Epoch: 442\n",
      "Epoch: 443\n",
      "Epoch: 444\n",
      "Epoch: 445\n",
      "Epoch: 446\n",
      "Epoch: 447\n",
      "Epoch: 448\n",
      "Epoch: 449\n",
      "Epoch: 450\n",
      "Epoch: 451\n",
      "Epoch: 452\n",
      "Epoch: 453\n",
      "Epoch: 454\n",
      "Epoch: 455\n",
      "Epoch: 456\n",
      "Epoch: 457\n",
      "Epoch: 458\n",
      "Epoch: 459\n",
      "Epoch: 460\n",
      "Epoch: 461\n",
      "Epoch: 462\n",
      "Epoch: 463\n",
      "Epoch: 464\n",
      "Epoch: 465\n",
      "Epoch: 466\n",
      "Epoch: 467\n",
      "Epoch: 468\n",
      "Epoch: 469\n",
      "Epoch: 470\n",
      "Epoch: 471\n",
      "Epoch: 472\n",
      "Epoch: 473\n",
      "Epoch: 474\n",
      "Epoch: 475\n",
      "Epoch: 476\n",
      "Epoch: 477\n",
      "Epoch: 478\n",
      "Epoch: 479\n",
      "Epoch: 480\n",
      "Epoch: 481\n",
      "Epoch: 482\n",
      "Epoch: 483\n",
      "Epoch: 484\n",
      "Epoch: 485\n",
      "Epoch: 486\n",
      "Epoch: 487\n",
      "Epoch: 488\n",
      "Epoch: 489\n",
      "Epoch: 490\n",
      "Epoch: 491\n",
      "Epoch: 492\n",
      "Epoch: 493\n",
      "Epoch: 494\n",
      "Epoch: 495\n",
      "Epoch: 496\n",
      "Epoch: 497\n",
      "Epoch: 498\n",
      "Epoch: 499\n",
      "Epoch: 500\n",
      "Epoch: 501\n",
      "Epoch: 502\n",
      "Epoch: 503\n",
      "Epoch: 504\n",
      "Epoch: 505\n",
      "Epoch: 506\n",
      "Epoch: 507\n",
      "Epoch: 508\n",
      "Epoch: 509\n",
      "Epoch: 510\n",
      "Epoch: 511\n",
      "Epoch: 512\n",
      "Epoch: 513\n",
      "Epoch: 514\n",
      "Epoch: 515\n",
      "Epoch: 516\n",
      "Epoch: 517\n",
      "Epoch: 518\n",
      "Epoch: 519\n",
      "Epoch: 520\n",
      "Epoch: 521\n",
      "Epoch: 522\n",
      "Epoch: 523\n",
      "Epoch: 524\n",
      "Epoch: 525\n",
      "Epoch: 526\n",
      "Epoch: 527\n",
      "Epoch: 528\n",
      "Epoch: 529\n",
      "Epoch: 530\n",
      "Epoch: 531\n",
      "Epoch: 532\n",
      "Epoch: 533\n",
      "Epoch: 534\n",
      "Epoch: 535\n",
      "Epoch: 536\n",
      "Epoch: 537\n",
      "Epoch: 538\n",
      "Epoch: 539\n",
      "Epoch: 540\n",
      "Epoch: 541\n",
      "Epoch: 542\n",
      "Epoch: 543\n",
      "Epoch: 544\n",
      "Epoch: 545\n",
      "Epoch: 546\n",
      "Epoch: 547\n",
      "Epoch: 548\n",
      "Epoch: 549\n",
      "Epoch: 550\n",
      "Epoch: 551\n",
      "Epoch: 552\n",
      "Epoch: 553\n",
      "Epoch: 554\n",
      "Epoch: 555\n",
      "Epoch: 556\n",
      "Epoch: 557\n",
      "Epoch: 558\n",
      "Epoch: 559\n",
      "Epoch: 560\n",
      "Epoch: 561\n",
      "Epoch: 562\n",
      "Epoch: 563\n",
      "Epoch: 564\n",
      "Epoch: 565\n",
      "Epoch: 566\n",
      "Epoch: 567\n",
      "Epoch: 568\n",
      "Epoch: 569\n",
      "Epoch: 570\n",
      "Epoch: 571\n",
      "Epoch: 572\n",
      "Epoch: 573\n",
      "Epoch: 574\n",
      "Epoch: 575\n",
      "Epoch: 576\n",
      "Epoch: 577\n",
      "Epoch: 578\n",
      "Epoch: 579\n",
      "Epoch: 580\n",
      "Epoch: 581\n",
      "Epoch: 582\n",
      "Epoch: 583\n",
      "Epoch: 584\n",
      "Epoch: 585\n",
      "Epoch: 586\n",
      "Epoch: 587\n",
      "Epoch: 588\n",
      "Epoch: 589\n",
      "Epoch: 590\n",
      "Epoch: 591\n",
      "Epoch: 592\n",
      "Epoch: 593\n",
      "Epoch: 594\n",
      "Epoch: 595\n",
      "Epoch: 596\n",
      "Epoch: 597\n",
      "Epoch: 598\n",
      "Epoch: 599\n",
      "Epoch: 600\n",
      "Epoch: 601\n",
      "Epoch: 602\n",
      "Epoch: 603\n",
      "Epoch: 604\n",
      "Epoch: 605\n",
      "Epoch: 606\n",
      "Epoch: 607\n",
      "Epoch: 608\n",
      "Epoch: 609\n",
      "Epoch: 610\n",
      "Epoch: 611\n",
      "Epoch: 612\n",
      "Epoch: 613\n",
      "Epoch: 614\n",
      "Epoch: 615\n",
      "Epoch: 616\n",
      "Epoch: 617\n",
      "Epoch: 618\n",
      "Epoch: 619\n",
      "Epoch: 620\n",
      "Epoch: 621\n",
      "Epoch: 622\n",
      "Epoch: 623\n",
      "Epoch: 624\n",
      "Epoch: 625\n",
      "Epoch: 626\n",
      "Epoch: 627\n",
      "Epoch: 628\n",
      "Epoch: 629\n",
      "Epoch: 630\n",
      "Epoch: 631\n",
      "Epoch: 632\n",
      "Epoch: 633\n",
      "Epoch: 634\n",
      "Epoch: 635\n",
      "Epoch: 636\n",
      "Epoch: 637\n",
      "Epoch: 638\n",
      "Epoch: 639\n",
      "Epoch: 640\n",
      "Epoch: 641\n",
      "Epoch: 642\n",
      "Epoch: 643\n",
      "Epoch: 644\n",
      "Epoch: 645\n",
      "Epoch: 646\n",
      "Epoch: 647\n",
      "Epoch: 648\n",
      "Epoch: 649\n",
      "Epoch: 650\n",
      "Epoch: 651\n",
      "Epoch: 652\n",
      "Epoch: 653\n",
      "Epoch: 654\n",
      "Epoch: 655\n",
      "Epoch: 656\n",
      "Epoch: 657\n",
      "Epoch: 658\n",
      "Epoch: 659\n",
      "Epoch: 660\n",
      "Epoch: 661\n",
      "Epoch: 662\n",
      "Epoch: 663\n",
      "Epoch: 664\n",
      "Epoch: 665\n",
      "Epoch: 666\n",
      "Epoch: 667\n",
      "Epoch: 668\n",
      "Epoch: 669\n",
      "Epoch: 670\n",
      "Epoch: 671\n",
      "Epoch: 672\n",
      "Epoch: 673\n",
      "Epoch: 674\n",
      "Epoch: 675\n",
      "Epoch: 676\n",
      "Epoch: 677\n",
      "Epoch: 678\n",
      "Epoch: 679\n",
      "Epoch: 680\n",
      "Epoch: 681\n",
      "Epoch: 682\n",
      "Epoch: 683\n",
      "Epoch: 684\n",
      "Epoch: 685\n",
      "Epoch: 686\n",
      "Epoch: 687\n",
      "Epoch: 688\n",
      "Epoch: 689\n",
      "Epoch: 690\n",
      "Epoch: 691\n",
      "Epoch: 692\n",
      "Epoch: 693\n",
      "Epoch: 694\n",
      "Epoch: 695\n",
      "Epoch: 696\n",
      "Epoch: 697\n",
      "Epoch: 698\n",
      "Epoch: 699\n",
      "Epoch: 700\n",
      "Epoch: 701\n",
      "Epoch: 702\n",
      "Epoch: 703\n",
      "Epoch: 704\n",
      "Epoch: 705\n",
      "Epoch: 706\n",
      "Epoch: 707\n",
      "Epoch: 708\n",
      "Epoch: 709\n",
      "Epoch: 710\n",
      "Epoch: 711\n",
      "Epoch: 712\n",
      "Epoch: 713\n",
      "Epoch: 714\n",
      "Epoch: 715\n",
      "Epoch: 716\n",
      "Epoch: 717\n",
      "Epoch: 718\n",
      "Epoch: 719\n",
      "Epoch: 720\n",
      "Epoch: 721\n",
      "Epoch: 722\n",
      "Epoch: 723\n",
      "Epoch: 724\n",
      "Epoch: 725\n",
      "Epoch: 726\n",
      "Epoch: 727\n",
      "Epoch: 728\n",
      "Epoch: 729\n",
      "Epoch: 730\n",
      "Epoch: 731\n",
      "Epoch: 732\n",
      "Epoch: 733\n",
      "Epoch: 734\n",
      "Epoch: 735\n",
      "Epoch: 736\n",
      "Epoch: 737\n",
      "Epoch: 738\n",
      "Epoch: 739\n",
      "Epoch: 740\n",
      "Epoch: 741\n",
      "Epoch: 742\n",
      "Epoch: 743\n",
      "Epoch: 744\n",
      "Epoch: 745\n",
      "Epoch: 746\n",
      "Epoch: 747\n",
      "Epoch: 748\n",
      "Epoch: 749\n",
      "Epoch: 750\n",
      "Epoch: 751\n",
      "Epoch: 752\n",
      "Epoch: 753\n",
      "Epoch: 754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 755\n",
      "Epoch: 756\n",
      "Epoch: 757\n",
      "Epoch: 758\n",
      "Epoch: 759\n",
      "Epoch: 760\n",
      "Epoch: 761\n",
      "Epoch: 762\n",
      "Epoch: 763\n",
      "Epoch: 764\n",
      "Epoch: 765\n",
      "Epoch: 766\n",
      "Epoch: 767\n",
      "Epoch: 768\n",
      "Epoch: 769\n",
      "Epoch: 770\n",
      "Epoch: 771\n",
      "Epoch: 772\n",
      "Epoch: 773\n",
      "Epoch: 774\n",
      "Epoch: 775\n",
      "Epoch: 776\n",
      "Epoch: 777\n",
      "Epoch: 778\n",
      "Epoch: 779\n",
      "Epoch: 780\n",
      "Epoch: 781\n",
      "Epoch: 782\n",
      "Epoch: 783\n",
      "Epoch: 784\n",
      "Epoch: 785\n",
      "Epoch: 786\n",
      "Epoch: 787\n",
      "Epoch: 788\n",
      "Epoch: 789\n",
      "Epoch: 790\n",
      "Epoch: 791\n",
      "Epoch: 792\n",
      "Epoch: 793\n",
      "Epoch: 794\n",
      "Epoch: 795\n",
      "Epoch: 796\n",
      "Epoch: 797\n",
      "Epoch: 798\n",
      "Epoch: 799\n",
      "Epoch: 800\n",
      "Epoch: 801\n",
      "Epoch: 802\n",
      "Epoch: 803\n",
      "Epoch: 804\n",
      "Epoch: 805\n",
      "Epoch: 806\n",
      "Epoch: 807\n",
      "Epoch: 808\n",
      "Epoch: 809\n",
      "Epoch: 810\n",
      "Epoch: 811\n",
      "Epoch: 812\n",
      "Epoch: 813\n",
      "Epoch: 814\n",
      "Epoch: 815\n",
      "Epoch: 816\n",
      "Epoch: 817\n",
      "Epoch: 818\n",
      "Epoch: 819\n",
      "Epoch: 820\n",
      "Epoch: 821\n",
      "Epoch: 822\n",
      "Epoch: 823\n",
      "Epoch: 824\n",
      "Epoch: 825\n",
      "Epoch: 826\n",
      "Epoch: 827\n",
      "Epoch: 828\n",
      "Epoch: 829\n",
      "Epoch: 830\n",
      "Epoch: 831\n",
      "Epoch: 832\n",
      "Epoch: 833\n",
      "Epoch: 834\n",
      "Epoch: 835\n",
      "Epoch: 836\n",
      "Epoch: 837\n",
      "Epoch: 838\n",
      "Epoch: 839\n",
      "Epoch: 840\n",
      "Epoch: 841\n",
      "Epoch: 842\n",
      "Epoch: 843\n",
      "Epoch: 844\n",
      "Epoch: 845\n",
      "Epoch: 846\n",
      "Epoch: 847\n",
      "Epoch: 848\n",
      "Epoch: 849\n",
      "Epoch: 850\n",
      "Epoch: 851\n",
      "Epoch: 852\n",
      "Epoch: 853\n",
      "Epoch: 854\n",
      "Epoch: 855\n",
      "Epoch: 856\n",
      "Epoch: 857\n",
      "Epoch: 858\n",
      "Epoch: 859\n",
      "Epoch: 860\n",
      "Epoch: 861\n",
      "Epoch: 862\n",
      "Epoch: 863\n",
      "Epoch: 864\n",
      "Epoch: 865\n",
      "Epoch: 866\n",
      "Epoch: 867\n",
      "Epoch: 868\n",
      "Epoch: 869\n",
      "Epoch: 870\n",
      "Epoch: 871\n",
      "Epoch: 872\n",
      "Epoch: 873\n",
      "Epoch: 874\n",
      "Epoch: 875\n",
      "Epoch: 876\n",
      "Epoch: 877\n",
      "Epoch: 878\n",
      "Epoch: 879\n",
      "Epoch: 880\n",
      "Epoch: 881\n",
      "Epoch: 882\n",
      "Epoch: 883\n",
      "Epoch: 884\n",
      "Epoch: 885\n",
      "Epoch: 886\n",
      "Epoch: 887\n",
      "Epoch: 888\n",
      "Epoch: 889\n",
      "Epoch: 890\n",
      "Epoch: 891\n",
      "Epoch: 892\n",
      "Epoch: 893\n",
      "Epoch: 894\n",
      "Epoch: 895\n",
      "Epoch: 896\n",
      "Epoch: 897\n",
      "Epoch: 898\n",
      "Epoch: 899\n",
      "Epoch: 900\n",
      "Epoch: 901\n",
      "Epoch: 902\n",
      "Epoch: 903\n",
      "Epoch: 904\n",
      "Epoch: 905\n",
      "Epoch: 906\n",
      "Epoch: 907\n",
      "Epoch: 908\n",
      "Epoch: 909\n",
      "Epoch: 910\n",
      "Epoch: 911\n",
      "Epoch: 912\n",
      "Epoch: 913\n",
      "Epoch: 914\n",
      "Epoch: 915\n",
      "Epoch: 916\n",
      "Epoch: 917\n",
      "Epoch: 918\n",
      "Epoch: 919\n",
      "Epoch: 920\n",
      "Epoch: 921\n",
      "Epoch: 922\n",
      "Epoch: 923\n",
      "Epoch: 924\n",
      "Epoch: 925\n",
      "Epoch: 926\n",
      "Epoch: 927\n",
      "Epoch: 928\n",
      "Epoch: 929\n",
      "Epoch: 930\n",
      "Epoch: 931\n",
      "Epoch: 932\n",
      "Epoch: 933\n",
      "Epoch: 934\n",
      "Epoch: 935\n",
      "Epoch: 936\n",
      "Epoch: 937\n",
      "Epoch: 938\n",
      "Epoch: 939\n",
      "Epoch: 940\n",
      "Epoch: 941\n",
      "Epoch: 942\n",
      "Epoch: 943\n",
      "Epoch: 944\n",
      "Epoch: 945\n",
      "Epoch: 946\n",
      "Epoch: 947\n",
      "Epoch: 948\n",
      "Epoch: 949\n",
      "Epoch: 950\n",
      "Epoch: 951\n",
      "Epoch: 952\n",
      "Epoch: 953\n",
      "Epoch: 954\n",
      "Epoch: 955\n",
      "Epoch: 956\n",
      "Epoch: 957\n",
      "Epoch: 958\n",
      "Epoch: 959\n",
      "Epoch: 960\n",
      "Epoch: 961\n",
      "Epoch: 962\n",
      "Epoch: 963\n",
      "Epoch: 964\n",
      "Epoch: 965\n",
      "Epoch: 966\n",
      "Epoch: 967\n",
      "Epoch: 968\n",
      "Epoch: 969\n",
      "Epoch: 970\n",
      "Epoch: 971\n",
      "Epoch: 972\n",
      "Epoch: 973\n",
      "Epoch: 974\n",
      "Epoch: 975\n",
      "Epoch: 976\n",
      "Epoch: 977\n",
      "Epoch: 978\n",
      "Epoch: 979\n",
      "Epoch: 980\n",
      "Epoch: 981\n",
      "Epoch: 982\n",
      "Epoch: 983\n",
      "Epoch: 984\n",
      "Epoch: 985\n",
      "Epoch: 986\n",
      "Epoch: 987\n",
      "Epoch: 988\n",
      "Epoch: 989\n",
      "Epoch: 990\n",
      "Epoch: 991\n",
      "Epoch: 992\n",
      "Epoch: 993\n",
      "Epoch: 994\n",
      "Epoch: 995\n",
      "Epoch: 996\n",
      "Epoch: 997\n",
      "Epoch: 998\n",
      "Epoch: 999\n"
     ]
    }
   ],
   "source": [
    "training_losses, n_in_epoch, alignments, xs = train_network(N_episodes, num_steps)\n",
    "all_losses.append(training_losses)\n",
    "all_alignments.append(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make some plots to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 10, 20, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = 5\n",
    "nB = 20\n",
    "theta_d = xs[:,:,:,:,0].reshape(N_epochs,-1, nB)\n",
    "theta = xs[:,:,:,:,1].reshape(N_epochs,-1,nB)\n",
    "h_d = xs[:,:,:,:,2].reshape(N_epochs,-1,nB)\n",
    "h = xs[:,:,:,:,3].reshape(N_epochs,-1,nB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853981633974483"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda2/envs/py34/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAFJCAYAAAAxJF5AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axis = plt.subplots(1,1,figsize = (5,5))\n",
    "#axis.plot(np.arange(5000),np.mean(theta<np.pi/8, 1))\n",
    "sns.tsplot(np.mean(theta<np.pi/8, 1).T, ax = axis, ci=68)\n",
    "axis.set_xlabel('trial #')\n",
    "axis.set_xlim([0, 5000])\n",
    "axis.set_ylabel('proportion of time upright')\n",
    "sns.despine(trim=True)\n",
    "plt.savefig('./figures/fig_3_cartpole_fa_partialobs.pdf')\n",
    "#plt.savefig('./figures/fig_3_cartpole_bp_partialobs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
