{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lansdell/projects/synthfeedback\n"
     ]
    }
   ],
   "source": [
    "cd /home/lansdell/projects/synthfeedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'\n",
    "import numpy as np\n",
    "import numpy.random as rand\n",
    "from numpy import random as rng\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l\n",
    "sns.set_style('ticks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cartpole RNN feedback alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using backprop\n",
      "For 1000 epochs\n",
      "Learning rate: 0.001000\n",
      "Lambda learning rate: 0.000050\n",
      "Variance xi: 0.500000\n",
      "Saving results: 0\n"
     ]
    }
   ],
   "source": [
    "#method = args.method\n",
    "#save = bool(args.save)\n",
    "\n",
    "method = 'backprop'\n",
    "save = False\n",
    "\n",
    "anneal = True\n",
    "\n",
    "# Global config variables\n",
    "num_steps = 10 # number of truncated backprop steps ('n' in the discussion above)\n",
    "batch_size = 20\n",
    "in_dim = 4\n",
    "state_size = 50\n",
    "learning_rate = 1e-3\n",
    "alpha2 = 1\n",
    "activation = tf.tanh\n",
    "act_prime = lambda x: 1 - tf.multiply(x,x)\n",
    "\n",
    "#Node pert params\n",
    "lmbda = 5e-5\n",
    "var_xi = 0.5\n",
    "p_fire = 0.1 #prob of firing\n",
    "\n",
    "acclimatize = True\n",
    "\n",
    "grad_max = 10\n",
    "\n",
    "N_epochs = 1000\n",
    "N_episodes = 10\n",
    "\n",
    "report_rate = 100\n",
    "fn_out = './experiments/cartpole_rnn_np/%s_learning_rate_%f_lmbda_%f_varxi_%f.npz'%(method, learning_rate, lmbda, var_xi)\n",
    "\n",
    "#Things to save with output\n",
    "params = {\n",
    "'num_steps': num_steps,\n",
    "'batch_size': batch_size,\n",
    "'in_dim': in_dim,\n",
    "'state_size': state_size,\n",
    "'learning_rate': learning_rate,\n",
    "'alpha2': alpha2,\n",
    "'lmbda': lmbda,\n",
    "'var_xi': var_xi,\n",
    "'p_fire': p_fire,\n",
    "'grad_max': grad_max,\n",
    "'N_epochs': N_epochs,\n",
    "'N_episodes': N_episodes,\n",
    "'acclimatize':acclimatize\n",
    "}\n",
    "\n",
    "#method = 'backprop'\n",
    "#method = 'feedbackalignment'\n",
    "#method = 'nodepert'\n",
    "#method = 'weightsym'\n",
    "\n",
    "print(\"Using %s\"%method)\n",
    "print(\"For %d epochs\"%N_epochs)\n",
    "print(\"Learning rate: %f\"%learning_rate)\n",
    "print(\"Lambda learning rate: %f\"%lmbda)\n",
    "print(\"Variance xi: %f\"%var_xi)\n",
    "print(\"Saving results: %d\"%save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell(rnn_input, state, W, U, B):\n",
    "    ones0 = tf.ones([batch_size, 1], tf.float32)\n",
    "    state_p = tf.concat([state, ones0], 1)\n",
    "    #return activation(tf.matmul(rnn_input, U) + tf.matmul(state_p, W))\n",
    "    return activation(tf.matmul(rnn_input, U) + tf_matmul_r(state_p, W, B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(num_episodes, num_steps, state_size=state_size, verbose=True):\n",
    "    xs = np.zeros((N_epochs, num_episodes, num_steps, batch_size, in_dim))\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        training_losses = []\n",
    "        alignments = []\n",
    "\n",
    "        for idx in range(N_epochs):\n",
    "            print(\"Epoch: %d\"%idx)\n",
    "            #if idx < 4 and acclimatize:\n",
    "            #    ts = train_step_B\n",
    "            #else:\n",
    "            ts = train_step\n",
    "            training_loss = 0\n",
    "            training_x = np.zeros((batch_size, in_dim))\n",
    "            #training_x = 2*rng.randn(batch_size, in_dim)\n",
    "            #training_x[:,1] = np.pi\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            for step in range(num_episodes):\n",
    "                tr_init_gradW = np.zeros((state_size+1, state_size))\n",
    "                tr_init_gradB = np.zeros((state_size+1, state_size))\n",
    "                tr_init_gradU = np.zeros((in_dim, state_size))\n",
    "                tr_loss, tr_losses, training_loss_, training_state, training_x, _, align, x_o = \\\n",
    "                    sess.run([loss, losses, total_loss, final_state, final_x, ts, aments, rnn_inputs], \\\n",
    "                                  feed_dict={init_state:training_state, init_x: training_x, \\\n",
    "                                  init_gradU: tr_init_gradU, init_gradW: tr_init_gradW, \\\n",
    "                                  init_gradB: tr_init_gradB})\n",
    "                #print(np.array(x_o).shape)\n",
    "                xs[idx, step, :, :, :] = np.array(x_o)[:,:,:]\n",
    "                training_loss += training_loss_\n",
    "                if step % report_rate == 0 and step > 0:\n",
    "                    if verbose:\n",
    "                        print(\"Average loss at step %d for last %d steps: %f\"%(step, report_rate, \\\n",
    "                                                                               training_loss/report_rate))\n",
    "                    training_losses.append(training_loss/report_rate)\n",
    "                    alignments.append(align)\n",
    "                    training_loss = 0\n",
    "\n",
    "    return training_losses, step, alignments, xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "## BACKPROP ##\n",
    "##############\n",
    "\n",
    "def backprop():\n",
    "    grad_U = init_gradU\n",
    "    grad_W = init_gradW\n",
    "    grad_B = init_gradB\n",
    "    alnments = []\n",
    "#     for i in range(num_steps):\n",
    "#         for j in range(i+1)[::-1]:\n",
    "#             if j == i:\n",
    "#                 delta = tf.multiply(tf.matmul(delta0s[i][:,None],tf.transpose(V[0:state_size,:])), \\\n",
    "#                                     act_prime(rnn_outputs[j]))\n",
    "#             else:\n",
    "#                 delta = tf.multiply(tf.matmul(delta, tf.transpose(W[0:state_size,:])), act_prime(rnn_outputs[j]))\n",
    "#             grad_U = grad_U + tf.matmul(tf.transpose(rnn_inputs[j]), delta)\n",
    "#             if j > 0:\n",
    "#                 grad_W = grad_W + tf.matmul(tf.transpose(tf.concat([rnn_outputs[j-1], ones0],1)), delta)\n",
    "    grad_V = tf.gradients(xs=V, ys=total_loss)[0]\n",
    "    grad_W = tf.gradients(xs=W, ys=total_loss)[0]\n",
    "    grad_U = tf.gradients(xs=U, ys=total_loss)[0]\n",
    "\n",
    "    return grad_U, grad_W, grad_B, grad_V, alnments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if method == 'backprop':\n",
    "    trainer = backprop\n",
    "elif method == 'feedbackalignment':\n",
    "    trainer = feedbackalignment\n",
    "elif method == 'nodepert':\n",
    "    trainer = nodepert\n",
    "elif method == 'weightsym':\n",
    "    trainer = weightsym\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "init_x = tf.zeros([batch_size, in_dim], dtype=np.float32)\n",
    "init_state = tf.zeros([batch_size, state_size], dtype=np.float32)\n",
    "init_gradW = tf.zeros([state_size+1, state_size], dtype=np.float32)\n",
    "init_gradB = tf.zeros([state_size+1, state_size], dtype=np.float32)\n",
    "init_gradU = tf.zeros([in_dim, state_size], dtype=np.float32)\n",
    "alignment = tf.zeros([in_dim, state_size], dtype=np.float32)\n",
    "\n",
    "ones0 = tf.ones([batch_size, 1], tf.float32)\n",
    "#U = tf.get_variable('U', [in_dim, state_size])\n",
    "#W = tf.get_variable('W', [state_size+1, state_size])\n",
    "#V = tf.get_variable('V', [state_size+1, 1])\n",
    "U = tf.Variable(rng.randn(in_dim, state_size)*alpha2, name=\"input_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(state_size+1, state_size)*alpha2, name=\"feedforward_weights\", dtype=tf.float32)\n",
    "#W = tf.Variable(np.zeros((state_size+1, state_size))*alpha2, name=\"feedforward_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(state_size+1, state_size)*alpha2, name=\"feedback_weights\", dtype=tf.float32)\n",
    "V = tf.Variable(rng.randn(state_size+1, 1)*alpha2, name=\"output_weights\", dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the pole task and dynamics here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = init_x\n",
    "state = init_state\n",
    "state_p = init_state\n",
    "rnn_inputs = []\n",
    "rnn_outputs = []\n",
    "rnn_pert_outputs = []\n",
    "noise_outputs = []\n",
    "heights = []\n",
    "hs = []\n",
    "actions = []\n",
    "\n",
    "#Cart pole dynamics simulated for num_steps \n",
    "m = 1.1\n",
    "mp = 0.1\n",
    "g = 9.8\n",
    "l = 0.5\n",
    "tau = 0.04\n",
    "Fmax = 10\n",
    "max_h = 3\n",
    "gamma = 10\n",
    "\n",
    "#Equations of motion:\n",
    "#theta_dd = (m*g*sin(theta) - cos(theta)*(F + mp*l*theta_d*theta_d*sin(theta)))/((4/3)*m*l - mp*l*cos(theta)*cos(theta))\n",
    "#theta_d += tau*theta_dd\n",
    "#theta += tau*theta\n",
    "#h_dd = (F + mp*l*(theta_d*theta_d*sin(theta)-theta_dd*cos(theta)))/m\n",
    "#h_d += tau*h_dd\n",
    "#h += tau*h_d\n",
    "\n",
    "for idx in range(num_steps):\n",
    "    mask = tf.random_uniform([batch_size, state_size]) < p_fire\n",
    "    xi = tf.multiply(tf.random_normal([batch_size, state_size])*var_xi, tf.to_float(mask))\n",
    "    phi = tf.random_normal((batch_size,1))*Fmax/5\n",
    "    #Compute new state\n",
    "    state = rnn_cell(x, state, W, U, B)\n",
    "    state_p = rnn_cell(x, state_p, W, U, B) + xi\n",
    "    #Compute action\n",
    "    action = tf.matmul(tf.concat([state, ones0], 1), V)\n",
    "    F = tf.squeeze(Fmax*activation(action) + phi)\n",
    "    #Compute new x\n",
    "    theta_dd = (m*g*tf.sin(x[:,1]) - tf.cos(x[:,1])*(F + mp*l*x[:,0]*x[:,0]*tf.sin(x[:,1])))/((4/3)*m*l -\\\n",
    "                mp*l*tf.cos(x[:,1])*tf.cos(x[:,1]))\n",
    "    h_dd = (F + mp*l*(x[:,0]*x[:,0]*tf.sin(x[:,1])-theta_dd*tf.cos(x[:,1])))/m\n",
    "    \n",
    "    #h_dd = (F - mp*l*x[:,0]*x[:,0]*tf.sin(x[:,1]) + mp*g*tf.sin(x[:,1])*tf.cos(x[:,1]))/(m - mp*tf.cos(x[:,1])*tf.cos(x[:,1]))\n",
    "    #theta_dd = (h_dd*tf.cos(x[:,1]) + g*tf.sin(x[:,1]))/l \n",
    "\n",
    "    x_list = []\n",
    "    x_list.append(x[:,0] + tau*theta_dd)   #x0 = theta_dot\n",
    "    x_list.append(x[:,1] + tau*x[:,0])     #x1 = theta\n",
    "    x_list.append(x[:,2] + tau*h_dd)       #x2 = h_dot\n",
    "    x_list.append(x[:,3] + tau*x[:,2])     #x3 = h\n",
    "    #x_list.append(tf.clip_by_value(x[:,3] + tau*x[:,2], -4*max_h, 4*max_h))     #x3 = h\n",
    "    x = tf.stack(x_list, axis = 1)\n",
    "    #height = tf.cos(x[:,1])\n",
    "    height = x[:,1]\n",
    "    heights.append(height)\n",
    "    hs.append(x[:,2])\n",
    "    rnn_inputs.append(x)\n",
    "    rnn_outputs.append(state)\n",
    "    rnn_pert_outputs.append(state_p)\n",
    "    noise_outputs.append(xi)\n",
    "    actions.append(action)\n",
    "    \n",
    "final_x = rnn_inputs[-1]\n",
    "final_state = rnn_outputs[-1]\n",
    "\n",
    "#Define loss function....\n",
    "#loss = \n",
    "\n",
    "loss = [gamma*tf.pow(height, 2)/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "losses = [gamma*tf.reduce_sum(tf.pow(height, 2))/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "\n",
    "#Perturbed outputs and loss\n",
    "loss_pert = [gamma*tf.pow(height, 2)/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "losses_pert = [gamma*tf.reduce_sum(tf.pow(height, 2))/2 + tf.pow(tf.maximum(0.0, tf.abs(h) - max_h),2)/2 for h, height in zip(hs, heights)]\n",
    "total_loss_pert = tf.reduce_mean(losses_pert)\n",
    "\n",
    "#e0s = [(height+1) for height in heights]\n",
    "#delta0s = e0s\n",
    "\n",
    "e0s = [tf.gradients(xs=actn, ys=lo)[0][:,0] for (actn,lo) in zip(actions, loss)]\n",
    "delta0s = e0s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_U, grad_W, grad_B, grad_V, aments = trainer()\n",
    "\n",
    "new_B = B.assign(B - lmbda*tf.clip_by_value(grad_B, -grad_max, grad_max, name=None))\n",
    "new_U = U.assign(U - learning_rate*grad_U)            \n",
    "new_W = W.assign(W - learning_rate*grad_W)           \n",
    "new_V = V.assign(V - learning_rate*grad_V)          \n",
    "\n",
    "train_step_B = [new_B]\n",
    "train_step = [new_U, new_W, new_V, new_B]\n",
    "\n",
    "all_losses = []\n",
    "all_alignments = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n",
      "Epoch: 101\n",
      "Epoch: 102\n",
      "Epoch: 103\n",
      "Epoch: 104\n",
      "Epoch: 105\n",
      "Epoch: 106\n",
      "Epoch: 107\n",
      "Epoch: 108\n",
      "Epoch: 109\n",
      "Epoch: 110\n",
      "Epoch: 111\n",
      "Epoch: 112\n",
      "Epoch: 113\n",
      "Epoch: 114\n",
      "Epoch: 115\n",
      "Epoch: 116\n",
      "Epoch: 117\n",
      "Epoch: 118\n",
      "Epoch: 119\n",
      "Epoch: 120\n",
      "Epoch: 121\n",
      "Epoch: 122\n",
      "Epoch: 123\n",
      "Epoch: 124\n",
      "Epoch: 125\n",
      "Epoch: 126\n",
      "Epoch: 127\n",
      "Epoch: 128\n",
      "Epoch: 129\n",
      "Epoch: 130\n",
      "Epoch: 131\n",
      "Epoch: 132\n",
      "Epoch: 133\n",
      "Epoch: 134\n",
      "Epoch: 135\n",
      "Epoch: 136\n",
      "Epoch: 137\n",
      "Epoch: 138\n",
      "Epoch: 139\n",
      "Epoch: 140\n",
      "Epoch: 141\n",
      "Epoch: 142\n",
      "Epoch: 143\n",
      "Epoch: 144\n",
      "Epoch: 145\n",
      "Epoch: 146\n",
      "Epoch: 147\n",
      "Epoch: 148\n",
      "Epoch: 149\n",
      "Epoch: 150\n",
      "Epoch: 151\n",
      "Epoch: 152\n",
      "Epoch: 153\n",
      "Epoch: 154\n",
      "Epoch: 155\n",
      "Epoch: 156\n",
      "Epoch: 157\n",
      "Epoch: 158\n",
      "Epoch: 159\n",
      "Epoch: 160\n",
      "Epoch: 161\n",
      "Epoch: 162\n",
      "Epoch: 163\n",
      "Epoch: 164\n",
      "Epoch: 165\n",
      "Epoch: 166\n",
      "Epoch: 167\n",
      "Epoch: 168\n",
      "Epoch: 169\n",
      "Epoch: 170\n",
      "Epoch: 171\n",
      "Epoch: 172\n",
      "Epoch: 173\n",
      "Epoch: 174\n",
      "Epoch: 175\n",
      "Epoch: 176\n",
      "Epoch: 177\n",
      "Epoch: 178\n",
      "Epoch: 179\n",
      "Epoch: 180\n",
      "Epoch: 181\n",
      "Epoch: 182\n",
      "Epoch: 183\n",
      "Epoch: 184\n",
      "Epoch: 185\n",
      "Epoch: 186\n",
      "Epoch: 187\n",
      "Epoch: 188\n",
      "Epoch: 189\n",
      "Epoch: 190\n",
      "Epoch: 191\n",
      "Epoch: 192\n",
      "Epoch: 193\n",
      "Epoch: 194\n",
      "Epoch: 195\n",
      "Epoch: 196\n",
      "Epoch: 197\n",
      "Epoch: 198\n",
      "Epoch: 199\n",
      "Epoch: 200\n",
      "Epoch: 201\n",
      "Epoch: 202\n",
      "Epoch: 203\n",
      "Epoch: 204\n",
      "Epoch: 205\n",
      "Epoch: 206\n",
      "Epoch: 207\n",
      "Epoch: 208\n",
      "Epoch: 209\n",
      "Epoch: 210\n",
      "Epoch: 211\n",
      "Epoch: 212\n",
      "Epoch: 213\n",
      "Epoch: 214\n",
      "Epoch: 215\n",
      "Epoch: 216\n",
      "Epoch: 217\n",
      "Epoch: 218\n",
      "Epoch: 219\n",
      "Epoch: 220\n",
      "Epoch: 221\n",
      "Epoch: 222\n",
      "Epoch: 223\n",
      "Epoch: 224\n",
      "Epoch: 225\n",
      "Epoch: 226\n",
      "Epoch: 227\n",
      "Epoch: 228\n",
      "Epoch: 229\n",
      "Epoch: 230\n",
      "Epoch: 231\n",
      "Epoch: 232\n",
      "Epoch: 233\n",
      "Epoch: 234\n",
      "Epoch: 235\n",
      "Epoch: 236\n",
      "Epoch: 237\n",
      "Epoch: 238\n",
      "Epoch: 239\n",
      "Epoch: 240\n",
      "Epoch: 241\n",
      "Epoch: 242\n",
      "Epoch: 243\n",
      "Epoch: 244\n",
      "Epoch: 245\n",
      "Epoch: 246\n",
      "Epoch: 247\n",
      "Epoch: 248\n",
      "Epoch: 249\n",
      "Epoch: 250\n",
      "Epoch: 251\n",
      "Epoch: 252\n",
      "Epoch: 253\n",
      "Epoch: 254\n",
      "Epoch: 255\n",
      "Epoch: 256\n",
      "Epoch: 257\n",
      "Epoch: 258\n",
      "Epoch: 259\n",
      "Epoch: 260\n",
      "Epoch: 261\n",
      "Epoch: 262\n",
      "Epoch: 263\n",
      "Epoch: 264\n",
      "Epoch: 265\n",
      "Epoch: 266\n",
      "Epoch: 267\n",
      "Epoch: 268\n",
      "Epoch: 269\n",
      "Epoch: 270\n",
      "Epoch: 271\n",
      "Epoch: 272\n",
      "Epoch: 273\n",
      "Epoch: 274\n",
      "Epoch: 275\n",
      "Epoch: 276\n",
      "Epoch: 277\n",
      "Epoch: 278\n",
      "Epoch: 279\n",
      "Epoch: 280\n",
      "Epoch: 281\n",
      "Epoch: 282\n",
      "Epoch: 283\n",
      "Epoch: 284\n",
      "Epoch: 285\n",
      "Epoch: 286\n",
      "Epoch: 287\n",
      "Epoch: 288\n",
      "Epoch: 289\n",
      "Epoch: 290\n",
      "Epoch: 291\n",
      "Epoch: 292\n",
      "Epoch: 293\n",
      "Epoch: 294\n",
      "Epoch: 295\n",
      "Epoch: 296\n",
      "Epoch: 297\n",
      "Epoch: 298\n",
      "Epoch: 299\n",
      "Epoch: 300\n",
      "Epoch: 301\n",
      "Epoch: 302\n",
      "Epoch: 303\n",
      "Epoch: 304\n",
      "Epoch: 305\n",
      "Epoch: 306\n",
      "Epoch: 307\n",
      "Epoch: 308\n",
      "Epoch: 309\n",
      "Epoch: 310\n",
      "Epoch: 311\n",
      "Epoch: 312\n",
      "Epoch: 313\n",
      "Epoch: 314\n",
      "Epoch: 315\n",
      "Epoch: 316\n",
      "Epoch: 317\n",
      "Epoch: 318\n",
      "Epoch: 319\n",
      "Epoch: 320\n",
      "Epoch: 321\n",
      "Epoch: 322\n",
      "Epoch: 323\n",
      "Epoch: 324\n",
      "Epoch: 325\n",
      "Epoch: 326\n",
      "Epoch: 327\n",
      "Epoch: 328\n",
      "Epoch: 329\n",
      "Epoch: 330\n",
      "Epoch: 331\n",
      "Epoch: 332\n",
      "Epoch: 333\n",
      "Epoch: 334\n",
      "Epoch: 335\n",
      "Epoch: 336\n",
      "Epoch: 337\n",
      "Epoch: 338\n",
      "Epoch: 339\n",
      "Epoch: 340\n",
      "Epoch: 341\n",
      "Epoch: 342\n",
      "Epoch: 343\n",
      "Epoch: 344\n",
      "Epoch: 345\n",
      "Epoch: 346\n",
      "Epoch: 347\n",
      "Epoch: 348\n",
      "Epoch: 349\n",
      "Epoch: 350\n",
      "Epoch: 351\n",
      "Epoch: 352\n",
      "Epoch: 353\n",
      "Epoch: 354\n",
      "Epoch: 355\n",
      "Epoch: 356\n",
      "Epoch: 357\n",
      "Epoch: 358\n",
      "Epoch: 359\n",
      "Epoch: 360\n",
      "Epoch: 361\n",
      "Epoch: 362\n",
      "Epoch: 363\n",
      "Epoch: 364\n",
      "Epoch: 365\n",
      "Epoch: 366\n",
      "Epoch: 367\n",
      "Epoch: 368\n",
      "Epoch: 369\n",
      "Epoch: 370\n",
      "Epoch: 371\n",
      "Epoch: 372\n",
      "Epoch: 373\n",
      "Epoch: 374\n",
      "Epoch: 375\n",
      "Epoch: 376\n",
      "Epoch: 377\n",
      "Epoch: 378\n",
      "Epoch: 379\n",
      "Epoch: 380\n",
      "Epoch: 381\n",
      "Epoch: 382\n",
      "Epoch: 383\n",
      "Epoch: 384\n",
      "Epoch: 385\n",
      "Epoch: 386\n",
      "Epoch: 387\n",
      "Epoch: 388\n",
      "Epoch: 389\n",
      "Epoch: 390\n",
      "Epoch: 391\n",
      "Epoch: 392\n",
      "Epoch: 393\n",
      "Epoch: 394\n",
      "Epoch: 395\n",
      "Epoch: 396\n",
      "Epoch: 397\n",
      "Epoch: 398\n",
      "Epoch: 399\n",
      "Epoch: 400\n",
      "Epoch: 401\n",
      "Epoch: 402\n",
      "Epoch: 403\n",
      "Epoch: 404\n",
      "Epoch: 405\n",
      "Epoch: 406\n",
      "Epoch: 407\n",
      "Epoch: 408\n",
      "Epoch: 409\n",
      "Epoch: 410\n",
      "Epoch: 411\n",
      "Epoch: 412\n",
      "Epoch: 413\n",
      "Epoch: 414\n",
      "Epoch: 415\n",
      "Epoch: 416\n",
      "Epoch: 417\n",
      "Epoch: 418\n",
      "Epoch: 419\n",
      "Epoch: 420\n",
      "Epoch: 421\n",
      "Epoch: 422\n",
      "Epoch: 423\n",
      "Epoch: 424\n",
      "Epoch: 425\n",
      "Epoch: 426\n",
      "Epoch: 427\n",
      "Epoch: 428\n",
      "Epoch: 429\n",
      "Epoch: 430\n",
      "Epoch: 431\n",
      "Epoch: 432\n",
      "Epoch: 433\n",
      "Epoch: 434\n",
      "Epoch: 435\n",
      "Epoch: 436\n",
      "Epoch: 437\n",
      "Epoch: 438\n",
      "Epoch: 439\n",
      "Epoch: 440\n",
      "Epoch: 441\n",
      "Epoch: 442\n",
      "Epoch: 443\n",
      "Epoch: 444\n",
      "Epoch: 445\n",
      "Epoch: 446\n",
      "Epoch: 447\n",
      "Epoch: 448\n",
      "Epoch: 449\n",
      "Epoch: 450\n",
      "Epoch: 451\n",
      "Epoch: 452\n",
      "Epoch: 453\n",
      "Epoch: 454\n",
      "Epoch: 455\n",
      "Epoch: 456\n",
      "Epoch: 457\n",
      "Epoch: 458\n",
      "Epoch: 459\n",
      "Epoch: 460\n",
      "Epoch: 461\n",
      "Epoch: 462\n",
      "Epoch: 463\n",
      "Epoch: 464\n",
      "Epoch: 465\n",
      "Epoch: 466\n",
      "Epoch: 467\n",
      "Epoch: 468\n",
      "Epoch: 469\n",
      "Epoch: 470\n",
      "Epoch: 471\n",
      "Epoch: 472\n",
      "Epoch: 473\n",
      "Epoch: 474\n",
      "Epoch: 475\n",
      "Epoch: 476\n",
      "Epoch: 477\n",
      "Epoch: 478\n",
      "Epoch: 479\n",
      "Epoch: 480\n",
      "Epoch: 481\n",
      "Epoch: 482\n",
      "Epoch: 483\n",
      "Epoch: 484\n",
      "Epoch: 485\n",
      "Epoch: 486\n",
      "Epoch: 487\n",
      "Epoch: 488\n",
      "Epoch: 489\n",
      "Epoch: 490\n",
      "Epoch: 491\n",
      "Epoch: 492\n",
      "Epoch: 493\n",
      "Epoch: 494\n",
      "Epoch: 495\n",
      "Epoch: 496\n",
      "Epoch: 497\n",
      "Epoch: 498\n",
      "Epoch: 499\n",
      "Epoch: 500\n",
      "Epoch: 501\n",
      "Epoch: 502\n",
      "Epoch: 503\n",
      "Epoch: 504\n",
      "Epoch: 505\n",
      "Epoch: 506\n",
      "Epoch: 507\n",
      "Epoch: 508\n",
      "Epoch: 509\n",
      "Epoch: 510\n",
      "Epoch: 511\n",
      "Epoch: 512\n",
      "Epoch: 513\n",
      "Epoch: 514\n",
      "Epoch: 515\n",
      "Epoch: 516\n",
      "Epoch: 517\n",
      "Epoch: 518\n",
      "Epoch: 519\n",
      "Epoch: 520\n",
      "Epoch: 521\n",
      "Epoch: 522\n",
      "Epoch: 523\n",
      "Epoch: 524\n",
      "Epoch: 525\n",
      "Epoch: 526\n",
      "Epoch: 527\n",
      "Epoch: 528\n",
      "Epoch: 529\n",
      "Epoch: 530\n",
      "Epoch: 531\n",
      "Epoch: 532\n",
      "Epoch: 533\n",
      "Epoch: 534\n",
      "Epoch: 535\n",
      "Epoch: 536\n",
      "Epoch: 537\n",
      "Epoch: 538\n",
      "Epoch: 539\n",
      "Epoch: 540\n",
      "Epoch: 541\n",
      "Epoch: 542\n",
      "Epoch: 543\n",
      "Epoch: 544\n",
      "Epoch: 545\n",
      "Epoch: 546\n",
      "Epoch: 547\n",
      "Epoch: 548\n",
      "Epoch: 549\n",
      "Epoch: 550\n",
      "Epoch: 551\n",
      "Epoch: 552\n",
      "Epoch: 553\n",
      "Epoch: 554\n",
      "Epoch: 555\n",
      "Epoch: 556\n",
      "Epoch: 557\n",
      "Epoch: 558\n",
      "Epoch: 559\n",
      "Epoch: 560\n",
      "Epoch: 561\n",
      "Epoch: 562\n",
      "Epoch: 563\n",
      "Epoch: 564\n",
      "Epoch: 565\n",
      "Epoch: 566\n",
      "Epoch: 567\n",
      "Epoch: 568\n",
      "Epoch: 569\n",
      "Epoch: 570\n",
      "Epoch: 571\n",
      "Epoch: 572\n",
      "Epoch: 573\n",
      "Epoch: 574\n",
      "Epoch: 575\n",
      "Epoch: 576\n",
      "Epoch: 577\n",
      "Epoch: 578\n",
      "Epoch: 579\n",
      "Epoch: 580\n",
      "Epoch: 581\n",
      "Epoch: 582\n",
      "Epoch: 583\n",
      "Epoch: 584\n",
      "Epoch: 585\n",
      "Epoch: 586\n",
      "Epoch: 587\n",
      "Epoch: 588\n",
      "Epoch: 589\n",
      "Epoch: 590\n",
      "Epoch: 591\n",
      "Epoch: 592\n",
      "Epoch: 593\n",
      "Epoch: 594\n",
      "Epoch: 595\n",
      "Epoch: 596\n",
      "Epoch: 597\n",
      "Epoch: 598\n",
      "Epoch: 599\n",
      "Epoch: 600\n",
      "Epoch: 601\n",
      "Epoch: 602\n",
      "Epoch: 603\n",
      "Epoch: 604\n",
      "Epoch: 605\n",
      "Epoch: 606\n",
      "Epoch: 607\n",
      "Epoch: 608\n",
      "Epoch: 609\n",
      "Epoch: 610\n",
      "Epoch: 611\n",
      "Epoch: 612\n",
      "Epoch: 613\n",
      "Epoch: 614\n",
      "Epoch: 615\n",
      "Epoch: 616\n",
      "Epoch: 617\n",
      "Epoch: 618\n",
      "Epoch: 619\n",
      "Epoch: 620\n",
      "Epoch: 621\n",
      "Epoch: 622\n",
      "Epoch: 623\n",
      "Epoch: 624\n",
      "Epoch: 625\n",
      "Epoch: 626\n",
      "Epoch: 627\n",
      "Epoch: 628\n",
      "Epoch: 629\n",
      "Epoch: 630\n",
      "Epoch: 631\n",
      "Epoch: 632\n",
      "Epoch: 633\n",
      "Epoch: 634\n",
      "Epoch: 635\n",
      "Epoch: 636\n",
      "Epoch: 637\n",
      "Epoch: 638\n",
      "Epoch: 639\n",
      "Epoch: 640\n",
      "Epoch: 641\n",
      "Epoch: 642\n",
      "Epoch: 643\n",
      "Epoch: 644\n",
      "Epoch: 645\n",
      "Epoch: 646\n",
      "Epoch: 647\n",
      "Epoch: 648\n",
      "Epoch: 649\n",
      "Epoch: 650\n",
      "Epoch: 651\n",
      "Epoch: 652\n",
      "Epoch: 653\n",
      "Epoch: 654\n",
      "Epoch: 655\n",
      "Epoch: 656\n",
      "Epoch: 657\n",
      "Epoch: 658\n",
      "Epoch: 659\n",
      "Epoch: 660\n",
      "Epoch: 661\n",
      "Epoch: 662\n",
      "Epoch: 663\n",
      "Epoch: 664\n",
      "Epoch: 665\n",
      "Epoch: 666\n",
      "Epoch: 667\n",
      "Epoch: 668\n",
      "Epoch: 669\n",
      "Epoch: 670\n",
      "Epoch: 671\n",
      "Epoch: 672\n",
      "Epoch: 673\n",
      "Epoch: 674\n",
      "Epoch: 675\n",
      "Epoch: 676\n",
      "Epoch: 677\n",
      "Epoch: 678\n",
      "Epoch: 679\n",
      "Epoch: 680\n",
      "Epoch: 681\n",
      "Epoch: 682\n",
      "Epoch: 683\n",
      "Epoch: 684\n",
      "Epoch: 685\n",
      "Epoch: 686\n",
      "Epoch: 687\n",
      "Epoch: 688\n",
      "Epoch: 689\n",
      "Epoch: 690\n",
      "Epoch: 691\n",
      "Epoch: 692\n",
      "Epoch: 693\n",
      "Epoch: 694\n",
      "Epoch: 695\n",
      "Epoch: 696\n",
      "Epoch: 697\n",
      "Epoch: 698\n",
      "Epoch: 699\n",
      "Epoch: 700\n",
      "Epoch: 701\n",
      "Epoch: 702\n",
      "Epoch: 703\n",
      "Epoch: 704\n",
      "Epoch: 705\n",
      "Epoch: 706\n",
      "Epoch: 707\n",
      "Epoch: 708\n",
      "Epoch: 709\n",
      "Epoch: 710\n",
      "Epoch: 711\n",
      "Epoch: 712\n",
      "Epoch: 713\n",
      "Epoch: 714\n",
      "Epoch: 715\n",
      "Epoch: 716\n",
      "Epoch: 717\n",
      "Epoch: 718\n",
      "Epoch: 719\n",
      "Epoch: 720\n",
      "Epoch: 721\n",
      "Epoch: 722\n",
      "Epoch: 723\n",
      "Epoch: 724\n",
      "Epoch: 725\n",
      "Epoch: 726\n",
      "Epoch: 727\n",
      "Epoch: 728\n",
      "Epoch: 729\n",
      "Epoch: 730\n",
      "Epoch: 731\n",
      "Epoch: 732\n",
      "Epoch: 733\n",
      "Epoch: 734\n",
      "Epoch: 735\n",
      "Epoch: 736\n",
      "Epoch: 737\n",
      "Epoch: 738\n",
      "Epoch: 739\n",
      "Epoch: 740\n",
      "Epoch: 741\n",
      "Epoch: 742\n",
      "Epoch: 743\n",
      "Epoch: 744\n",
      "Epoch: 745\n",
      "Epoch: 746\n",
      "Epoch: 747\n",
      "Epoch: 748\n",
      "Epoch: 749\n",
      "Epoch: 750\n",
      "Epoch: 751\n",
      "Epoch: 752\n",
      "Epoch: 753\n",
      "Epoch: 754\n",
      "Epoch: 755\n",
      "Epoch: 756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 757\n",
      "Epoch: 758\n",
      "Epoch: 759\n",
      "Epoch: 760\n",
      "Epoch: 761\n",
      "Epoch: 762\n",
      "Epoch: 763\n",
      "Epoch: 764\n",
      "Epoch: 765\n",
      "Epoch: 766\n",
      "Epoch: 767\n",
      "Epoch: 768\n",
      "Epoch: 769\n",
      "Epoch: 770\n",
      "Epoch: 771\n",
      "Epoch: 772\n",
      "Epoch: 773\n",
      "Epoch: 774\n",
      "Epoch: 775\n",
      "Epoch: 776\n",
      "Epoch: 777\n",
      "Epoch: 778\n",
      "Epoch: 779\n",
      "Epoch: 780\n",
      "Epoch: 781\n",
      "Epoch: 782\n",
      "Epoch: 783\n",
      "Epoch: 784\n",
      "Epoch: 785\n",
      "Epoch: 786\n",
      "Epoch: 787\n",
      "Epoch: 788\n",
      "Epoch: 789\n",
      "Epoch: 790\n",
      "Epoch: 791\n",
      "Epoch: 792\n",
      "Epoch: 793\n",
      "Epoch: 794\n",
      "Epoch: 795\n",
      "Epoch: 796\n",
      "Epoch: 797\n",
      "Epoch: 798\n",
      "Epoch: 799\n",
      "Epoch: 800\n",
      "Epoch: 801\n",
      "Epoch: 802\n",
      "Epoch: 803\n",
      "Epoch: 804\n",
      "Epoch: 805\n",
      "Epoch: 806\n",
      "Epoch: 807\n",
      "Epoch: 808\n",
      "Epoch: 809\n",
      "Epoch: 810\n",
      "Epoch: 811\n",
      "Epoch: 812\n",
      "Epoch: 813\n",
      "Epoch: 814\n",
      "Epoch: 815\n",
      "Epoch: 816\n",
      "Epoch: 817\n",
      "Epoch: 818\n",
      "Epoch: 819\n",
      "Epoch: 820\n",
      "Epoch: 821\n",
      "Epoch: 822\n",
      "Epoch: 823\n",
      "Epoch: 824\n",
      "Epoch: 825\n",
      "Epoch: 826\n",
      "Epoch: 827\n",
      "Epoch: 828\n",
      "Epoch: 829\n",
      "Epoch: 830\n",
      "Epoch: 831\n",
      "Epoch: 832\n",
      "Epoch: 833\n",
      "Epoch: 834\n",
      "Epoch: 835\n",
      "Epoch: 836\n",
      "Epoch: 837\n",
      "Epoch: 838\n",
      "Epoch: 839\n",
      "Epoch: 840\n",
      "Epoch: 841\n",
      "Epoch: 842\n",
      "Epoch: 843\n",
      "Epoch: 844\n",
      "Epoch: 845\n",
      "Epoch: 846\n",
      "Epoch: 847\n",
      "Epoch: 848\n",
      "Epoch: 849\n",
      "Epoch: 850\n",
      "Epoch: 851\n",
      "Epoch: 852\n",
      "Epoch: 853\n",
      "Epoch: 854\n",
      "Epoch: 855\n",
      "Epoch: 856\n",
      "Epoch: 857\n",
      "Epoch: 858\n",
      "Epoch: 859\n",
      "Epoch: 860\n",
      "Epoch: 861\n",
      "Epoch: 862\n",
      "Epoch: 863\n",
      "Epoch: 864\n",
      "Epoch: 865\n",
      "Epoch: 866\n",
      "Epoch: 867\n",
      "Epoch: 868\n",
      "Epoch: 869\n",
      "Epoch: 870\n",
      "Epoch: 871\n",
      "Epoch: 872\n",
      "Epoch: 873\n",
      "Epoch: 874\n",
      "Epoch: 875\n",
      "Epoch: 876\n",
      "Epoch: 877\n",
      "Epoch: 878\n",
      "Epoch: 879\n",
      "Epoch: 880\n",
      "Epoch: 881\n",
      "Epoch: 882\n",
      "Epoch: 883\n",
      "Epoch: 884\n",
      "Epoch: 885\n",
      "Epoch: 886\n",
      "Epoch: 887\n",
      "Epoch: 888\n",
      "Epoch: 889\n",
      "Epoch: 890\n",
      "Epoch: 891\n",
      "Epoch: 892\n",
      "Epoch: 893\n",
      "Epoch: 894\n",
      "Epoch: 895\n",
      "Epoch: 896\n",
      "Epoch: 897\n",
      "Epoch: 898\n",
      "Epoch: 899\n",
      "Epoch: 900\n",
      "Epoch: 901\n",
      "Epoch: 902\n",
      "Epoch: 903\n",
      "Epoch: 904\n",
      "Epoch: 905\n",
      "Epoch: 906\n",
      "Epoch: 907\n",
      "Epoch: 908\n",
      "Epoch: 909\n",
      "Epoch: 910\n",
      "Epoch: 911\n",
      "Epoch: 912\n",
      "Epoch: 913\n",
      "Epoch: 914\n",
      "Epoch: 915\n",
      "Epoch: 916\n",
      "Epoch: 917\n",
      "Epoch: 918\n",
      "Epoch: 919\n",
      "Epoch: 920\n",
      "Epoch: 921\n",
      "Epoch: 922\n",
      "Epoch: 923\n",
      "Epoch: 924\n",
      "Epoch: 925\n",
      "Epoch: 926\n",
      "Epoch: 927\n",
      "Epoch: 928\n",
      "Epoch: 929\n",
      "Epoch: 930\n",
      "Epoch: 931\n",
      "Epoch: 932\n",
      "Epoch: 933\n",
      "Epoch: 934\n",
      "Epoch: 935\n",
      "Epoch: 936\n",
      "Epoch: 937\n",
      "Epoch: 938\n",
      "Epoch: 939\n",
      "Epoch: 940\n",
      "Epoch: 941\n",
      "Epoch: 942\n",
      "Epoch: 943\n",
      "Epoch: 944\n",
      "Epoch: 945\n",
      "Epoch: 946\n",
      "Epoch: 947\n",
      "Epoch: 948\n",
      "Epoch: 949\n",
      "Epoch: 950\n",
      "Epoch: 951\n",
      "Epoch: 952\n",
      "Epoch: 953\n",
      "Epoch: 954\n",
      "Epoch: 955\n",
      "Epoch: 956\n",
      "Epoch: 957\n",
      "Epoch: 958\n",
      "Epoch: 959\n",
      "Epoch: 960\n",
      "Epoch: 961\n",
      "Epoch: 962\n",
      "Epoch: 963\n",
      "Epoch: 964\n",
      "Epoch: 965\n",
      "Epoch: 966\n",
      "Epoch: 967\n",
      "Epoch: 968\n",
      "Epoch: 969\n",
      "Epoch: 970\n",
      "Epoch: 971\n",
      "Epoch: 972\n",
      "Epoch: 973\n",
      "Epoch: 974\n",
      "Epoch: 975\n",
      "Epoch: 976\n",
      "Epoch: 977\n",
      "Epoch: 978\n",
      "Epoch: 979\n",
      "Epoch: 980\n",
      "Epoch: 981\n",
      "Epoch: 982\n",
      "Epoch: 983\n",
      "Epoch: 984\n",
      "Epoch: 985\n",
      "Epoch: 986\n",
      "Epoch: 987\n",
      "Epoch: 988\n",
      "Epoch: 989\n",
      "Epoch: 990\n",
      "Epoch: 991\n",
      "Epoch: 992\n",
      "Epoch: 993\n",
      "Epoch: 994\n",
      "Epoch: 995\n",
      "Epoch: 996\n",
      "Epoch: 997\n",
      "Epoch: 998\n",
      "Epoch: 999\n"
     ]
    }
   ],
   "source": [
    "training_losses, n_in_epoch, alignments, xs = train_network(N_episodes, num_steps)\n",
    "all_losses.append(training_losses)\n",
    "all_alignments.append(alignments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make some plots to debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10, 10, 20, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = 5\n",
    "nB = 20\n",
    "theta_d = xs[:,:,:,:,0].reshape(N_epochs,-1, nB)\n",
    "theta = xs[:,:,:,:,1].reshape(N_epochs,-1,nB)\n",
    "h_d = xs[:,:,:,:,2].reshape(N_epochs,-1,nB)\n",
    "h = xs[:,:,:,:,3].reshape(N_epochs,-1,nB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7853981633974483"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.pi/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda2/envs/py34/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAFJCAYAAAAxJF5AAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMmLIEhHJiCFLRCQjhiwRkYwYskREMvr/f1YcVoixd/gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axis = plt.subplots(1,1,figsize = (5,5))\n",
    "#axis.plot(np.arange(5000),np.mean(theta<np.pi/8, 1))\n",
    "sns.tsplot(np.mean(theta<np.pi/8, 1).T, ax = axis, ci=68)\n",
    "axis.set_xlabel('trial #')\n",
    "axis.set_xlim([0, 5000])\n",
    "axis.set_ylabel('proportion of time upright')\n",
    "sns.despine(trim=True)\n",
    "#plt.savefig('./figues/fig_3_cartpole_bp.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
