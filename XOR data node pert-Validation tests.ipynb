{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "from data_loader.data_generator import MNISTDataGenerator, LinearDataGenerator\n",
    "from models.npmodels import NPModel4,DirectNPModel4,AENPModel,AEDFANPModel\n",
    "from trainers.sf_trainer import SFTrainer, AESFTrainer\n",
    "from utils.config import process_config\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l, tf_eigvecs, tf_eigvals\n",
    "\n",
    "import itertools\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# inshape=30\n",
    "# hidden=20\n",
    "# outshape=10\n",
    "batch_size=50\n",
    "# T = rng.randn(outshape, inshape)\n",
    "def traindata(batch_size):\n",
    "    train_x=np.zeros((batch_size,2))\n",
    "    train_y=np.zeros((batch_size))\n",
    "    for i in range(batch_size):\n",
    "        inp=np.random.randint(0,2,size=[1,2])\n",
    "        output=operator.xor(inp[0,0],inp[0,1])\n",
    "        train_x[i,:]=inp\n",
    "        train_y[i]=output\n",
    "    return (train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#p = self.config.state_size[0]\n",
    "p=2# inshape \n",
    "m =10# hiddenshap\n",
    "j = 1#outshpae\n",
    "#n = 10\n",
    "var_xi = 0.1\n",
    "# learning_rate=0\n",
    "# lmda_learning_rate=1e-5\n",
    "#lmda_learning_rate=0\n",
    "\n",
    "#Training data inputs\n",
    "x=tf.placeholder(tf.float32,[None,p], name = 'x')\n",
    "y=tf.placeholder(tf.float32,[None,j], name = 'y')\n",
    "learning_rate=tf.placeholder(tf.float32,[None],name='learning_rate')\n",
    "lmda_learning_rate=tf.placeholder(tf.float32,[None],name='lmda_learning_rate')\n",
    "\n",
    "#Scale weight initialization\n",
    "alpha0 = np.sqrt(2.0/p)\n",
    "alpha1 = np.sqrt(2.0/m)\n",
    "alpha2 = np.sqrt(2.0/j)\n",
    "alpha3 = 1\n",
    "\n",
    "A = tf.Variable(rng.randn(p+1,m)*alpha0, name=\"hidden_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(m+1,j)*alpha1, name=\"output_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(m+1,j)*alpha2, name=\"feedback_weights\", dtype=tf.float32)\n",
    "\n",
    "# network architecture with ones added for bias terms\n",
    "e0 = tf.ones([batch_size, 1], tf.float32)\n",
    "e1 = tf.ones([batch_size, 1], tf.float32)\n",
    "x_aug = tf.concat([x, e0], 1)\n",
    "h = tf.sigmoid(tf.matmul(x_aug, A))\n",
    "#Make some noise\n",
    "h_aug = tf.concat([h, e1], 1)\n",
    "xi = tf.random_normal(shape=tf.shape(h_aug), mean=0.0, stddev=var_xi, dtype=tf.float32)\n",
    "h_tilde = h_aug + xi\n",
    "#Add noise to hidden layer\n",
    "y_p = tf.matmul(h_tilde, W)\n",
    "y_p_0 = tf.matmul(h_aug, W)\n",
    "\n",
    "trainable = [A, W, B]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mean squared error\n",
    "loss = tf.reduce_sum(tf.pow(y_p-y, 2))/2\n",
    "loss_0 = tf.reduce_sum(tf.pow(y_p_0-y, 2))/2\n",
    "e = (y_p - y)\n",
    "h_prime = tf.multiply(h_tilde, 1-h_tilde)[:,0:m]\n",
    "\n",
    "#Feedback data for saving\n",
    "#Only take first item in epoch\n",
    "delta_bp = tf.matmul(e, tf.transpose(W[0:m,:]))[0,:]\n",
    "delta_fa = tf.matmul(e, tf.transpose(B[0:m,:]))[0,:]\n",
    "norm_W = tf.norm(W)\n",
    "norm_B = tf.norm(B)\n",
    "error_FA = tf.norm(delta_bp - delta_fa)\n",
    "alignment = tf.reduce_sum(tf.multiply(delta_fa,delta_bp))/tf.norm(delta_fa)/tf.norm(delta_bp)\n",
    "norm_diff = tf.norm(W - B)\n",
    "eigs = tf_eigvals(tf.matmul(tf.transpose(B), W))\n",
    "\n",
    "#Compute updates for W and A (based on B)\n",
    "lmda = tf.matmul(e, tf.transpose(B[0:m,:]))\n",
    "grad_W = tf.gradients(xs=W, ys=loss)[0]\n",
    "grad_A = tf.matmul(tf.transpose(x_aug), tf.multiply(h_prime, lmda))\n",
    "grad_B = tf.matmul(tf.matmul(B, tf.transpose(e)) - tf.transpose(xi)*(loss - loss_0)/var_xi, e)\n",
    "\n",
    "new_W = W.assign(W - learning_rate*grad_W)\n",
    "new_A = A.assign(A - learning_rate*grad_A)            \n",
    "new_B = B.assign(B - lmda_learning_rate*grad_B)\n",
    "train_step = [new_W, new_A, new_B]\n",
    "\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(y_p, 1), tf.argmax(y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# #Also need to add eigenvector stuff\n",
    "# training_metrics = [alignment, norm_W, norm_B, error_FA, eigs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present combo: 1\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 2\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 3\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 4\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 5\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 6\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 7\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 8\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 9\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 10\n",
      "Present learning rate: 1e-06\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 11\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 12\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 13\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 14\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 15\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 16\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 17\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 18\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 19\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 20\n",
      "Present learning rate: 2.1544346900318865e-06\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 21\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 22\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 23\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 24\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 25\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 26\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 27\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 28\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 29\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 30\n",
      "Present learning rate: 4.641588833612782e-06\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 31\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 32\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 33\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 34\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 35\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 36\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 37\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 38\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 39\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 40\n",
      "Present learning rate: 1e-05\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 41\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 42\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 43\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 44\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 45\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 46\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 47\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 48\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 49\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 50\n",
      "Present learning rate: 2.1544346900318867e-05\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 51\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 52\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 53\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 54\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 55\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 56\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 57\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 58\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 59\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 60\n",
      "Present learning rate: 4.641588833612782e-05\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 61\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 62\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 63\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 64\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 65\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 66\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 67\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 68\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 69\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 70\n",
      "Present learning rate: 0.0001\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 71\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 72\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 73\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 74\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 75\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 76\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 77\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 78\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 79\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 80\n",
      "Present learning rate: 0.00021544346900318845\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 81\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 1e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Present combo: 82\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 83\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 84\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 85\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 86\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 87\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 88\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 0.00021544346900318845\n",
      "Present combo: 89\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 0.00046415888336127773\n",
      "Present combo: 90\n",
      "Present learning rate: 0.00046415888336127773\n",
      "Present lmda learning rate: 0.001\n",
      "Present combo: 91\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 1e-06\n",
      "Present combo: 92\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 2.1544346900318865e-06\n",
      "Present combo: 93\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 4.641588833612782e-06\n",
      "Present combo: 94\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 1e-05\n",
      "Present combo: 95\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 2.1544346900318867e-05\n",
      "Present combo: 96\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 4.641588833612782e-05\n",
      "Present combo: 97\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 0.0001\n",
      "Present combo: 98\n",
      "Present learning rate: 0.001\n",
      "Present lmda learning rate: 0.00021544346900318845\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "eta_1=np.logspace(-3,-6,10)\n",
    "# eta_1=[0.1,0.1,0.1,0.1,0.1,0.1]\n",
    "eta_lmda=np.logspace(-3,-6,10)\n",
    "combo=list(itertools.product(eta_1[::-1],eta_lmda[::-1]))\n",
    "iteration=10000\n",
    "store_al=np.zeros([len(combo),iteration])\n",
    "store_df=np.zeros([len(combo),iteration])\n",
    "# store_err=[[] for i in range(len(combo))]\n",
    "store_err=np.zeros([len(combo),iteration])\n",
    "non_converge=[]\n",
    "accuracy=np.zeros([len(combo),iteration])\n",
    "with tf.Session() as sess:\n",
    "    for i in range(len(combo)):\n",
    "        sess.run(init)\n",
    "        print(\"Present combo:\",i+1)\n",
    "        print(\"Present learning rate:\",combo[i][0])\n",
    "        learning_rate1=[combo[i][0]]\n",
    "        lmda_learning_rate1=[combo[i][1]]\n",
    "        print(\"Present lmda learning rate:\",combo[i][1])  \n",
    "        flag=0\n",
    "        for idx in range(iteration):\n",
    "            (train_x, train_y) = traindata(batch_size)\n",
    "            train_x=np.float32(train_x)\n",
    "            train_y=(np.float32(train_y))\n",
    "            train_y=train_y.reshape((batch_size,1))#to make it [50,1] instead of [50,]\n",
    "            _,align,diff,err,out=sess.run([train_step,alignment,norm_diff,loss_0,y_p],feed_dict={x: train_x, y: train_y,\n",
    "                                                                                        learning_rate:learning_rate1,\n",
    "                                                                                         lmda_learning_rate:lmda_learning_rate1})\n",
    "            if flag==0:\n",
    "                if np.isnan(err)==True:\n",
    "                    print(\"\\n\\tModel does not converge!!!\\n\")\n",
    "                    non_converge.append(i)\n",
    "                    flag=1\n",
    "                    break\n",
    "            err_per=np.sum(np.abs(train_y-np.round(abs(out))))/batch_size      \n",
    "            accuracy[i,idx]=(err_per)\n",
    "            store_al[i,idx]=align\n",
    "            store_df[i,idx]=diff\n",
    "#             store_err[i].append(err)\n",
    "            store_err[i,idx]=err\n",
    "#             if (idx+1)%500==0:\n",
    "#                 print(\"Iteration count:\",idx+1)\n",
    "\n",
    "# for rem in non_converge:\n",
    "#     del store_err[rem]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_acc=accuracy[:,-1]\n",
    "\n",
    "reshape_acc=np.reshape(last_acc,[eta_1.shape[0],eta_lmda.shape[0]])\n",
    "print(last_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(eta_1)\n",
    "print(eta_lmda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store_err_s=[]\n",
    "store_al_s=[]\n",
    "store_df_s=[]\n",
    "print(\"Following indexes did not converge\",non_converge)\n",
    "for count in range(store_err.shape[0]):\n",
    "    if count in non_converge:\n",
    "        print(\"removing row:\",count)\n",
    "    else:\n",
    "        store_err_s.append(store_err[count,:]) \n",
    "#         store_al_s.append(store_al[count,:])\n",
    "\n",
    "\n",
    "store_err_s=np.array(store_err_s)\n",
    "\n",
    "for val in sorted(non_converge,reverse=True):\n",
    "    del combo[val]\n",
    "eta_1new=[]\n",
    "eta_lmdanew=[]\n",
    "for first,second in combo:\n",
    "    if first not in eta_1new:\n",
    "        eta_1new.append(first)\n",
    "    if second not in eta_lmdanew:\n",
    "        eta_lmdanew.append(second)\n",
    "eta_1new=np.array(eta_1new)\n",
    "eta_lmdanew=np.array(eta_lmdanew)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_lmdanew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#storing last values for error and reshaping into appropriate format\n",
    "last_val=np.zeros([eta_1new.shape[0]*eta_lmdanew.shape[0],1])\n",
    "for num in range(store_err_s.shape[0]):\n",
    "    last_val[num]=store_err_s[num][-1]\n",
    "reshape_err=np.reshape(last_val,[eta_1new.shape[0],eta_lmdanew.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lf_df=np.zeros([eta_1.shape[0]*eta_lmda.shape[0],10]) #stores last 10 norm_dfs\n",
    "mean_df=np.zeros([eta_1.shape[0]*eta_lmda.shape[0],1]) #stores mean of last 10 norm_dfs\n",
    "for num in range(store_df.shape[0]):\n",
    "    lf_df[num,:]=store_df[num,iteration-10:iteration]\n",
    "    mean_df[num]=np.mean(store_df[num,iteration-10:iteration])\n",
    "    \n",
    "reshape_df=np.reshape(mean_df,[eta_1.shape[0],eta_lmda.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_al=np.zeros([eta_1.shape[0]*eta_lmda.shape[0],1])\n",
    "for num2 in range(store_al.shape[0]):\n",
    "    last_al[num2]=store_al[num2][-1]\n",
    "\n",
    "reshape_al=np.reshape(last_al,[eta_1.shape[0],eta_lmda.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta_xtick=eta_1new\n",
    "for x in range(len(eta_1new)):\n",
    "    eta_xtick[x]=\"%0.1e\" % eta_1new[x]\n",
    "eta_ytick=eta_lmdanew\n",
    "for y in range(len(eta_lmdanew)):\n",
    "    eta_ytick[y]=\"%0.1e\" % eta_lmdanew[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(10,10))\n",
    "# plt.axes(xscale='log',yscale='log')\n",
    "# ax.locator_params(axis='y',nbins=len(eta_ytick))\n",
    "# ax.locator_params(axis='x',nbins=len(eta_xtick))\n",
    "h=ax.imshow(reshape_err, origin='lower',cmap='jet',interpolation='none')\n",
    "plt.colorbar(h,ax=ax)\n",
    "\n",
    "# .MaxNLocator(3)\n",
    "ax.set_xticklabels([str(eta) for eta in eta_xtick])\n",
    "ax.set_yticklabels([str(eta) for eta in eta_ytick])\n",
    "\n",
    "\n",
    "# ax.set_xticks(strings)\n",
    "# start, end = ax.get_xlim()\n",
    "# ax.xaxis.set_ticks(np.arange(start, end, 0.712123))\n",
    "# ax.xaxis.set_major_formatter(ticker.FormatStrFormatter('%0.01e'))\n",
    "\n",
    "ax.locator_params(axis='x',nbins=len(eta_xtick))\n",
    "ax.locator_params(axis='y',nbins=len(eta_ytick))\n",
    "\n",
    "\n",
    "# ax.set_ticks(eta_xtick)\n",
    "\n",
    "ax.set_xlabel(\"learning_rate \")\n",
    "ax.set_ylabel(\"lmda_learning_rate\")\n",
    "plt.title(\"Heatmap for loss at the end of all iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_xtick1=eta_1[::-1]\n",
    "for x in range(len(eta_1)):\n",
    "    eta_xtick1[x]=\"%0.1e\" % eta_1[x]\n",
    "eta_ytick1=eta_lmda[::-1]\n",
    "for y in range(len(eta_lmda)):\n",
    "    eta_ytick1[y]=\"%0.1e\" % eta_lmda[y]\n",
    "    \n",
    "    \n",
    "fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "h2=plt.imshow(reshape_al,cmap='jet',origin='lower')\n",
    "plt.colorbar(h2,ax=ax)\n",
    "ax.locator_params(axis='x',nbins=len(eta_xtick1))\n",
    "ax.locator_params(axis='y',nbins=len(eta_ytick1))\n",
    "ax.set_xticklabels([str(eta) for eta in eta_xtick1])\n",
    "ax.set_yticklabels([str(eta) for eta in eta_ytick1])\n",
    "\n",
    "\n",
    "plt.title(\"Heat map of alignaments for all learning rates\")\n",
    "ax.set_xlabel(\"learning_rate\")\n",
    "ax.set_ylabel(\"lmda_learning_rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_1[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_xtick1=eta_1[::-1]\n",
    "for x in range(len(eta_1)):\n",
    "    eta_xtick1[x]=\"%0.1e\" % eta_1[x]\n",
    "eta_ytick1=eta_lmda[::-1]\n",
    "for y in range(len(eta_lmda)):\n",
    "    eta_ytick1[y]=\"%0.1e\" % eta_lmda[y]\n",
    "    \n",
    "    \n",
    "fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "h2=plt.imshow(reshape_acc,cmap='jet',origin='lower')\n",
    "plt.colorbar(h2,ax=ax)\n",
    "ax.locator_params(axis='x',nbins=len(eta_xtick1))\n",
    "ax.locator_params(axis='y',nbins=len(eta_ytick1))\n",
    "ax.set_xticklabels([str(eta) for eta in eta_xtick1])\n",
    "ax.set_yticklabels([str(eta) for eta in eta_ytick1])\n",
    "\n",
    "\n",
    "plt.title(\"Heat map of errors for all learning rates\")\n",
    "ax.set_xlabel(\"learning_rate\")\n",
    "ax.set_ylabel(\"lmda_learning_rate\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(1,1,figsize=(10,10))\n",
    "h3=plt.imshow(reshape_df,cmap='jet',origin='lower')\n",
    "plt.colorbar(h3,ax=ax)\n",
    "\n",
    "ax.set_xticklabels([str(eta) for eta in eta_xtick1])\n",
    "ax.set_yticklabels([str(eta) for eta in eta_ytick1])\n",
    "ax.locator_params(axis='x',nbins=len(eta_xtick1))\n",
    "ax.locator_params(axis='y',nbins=len(eta_ytick1))\n",
    "\n",
    "plt.title(\"Heatmap of mean of the the last 10 norms differences\")\n",
    "plt.xlabel(\"learning_rate\")\n",
    "plt.ylabel(\"lmda_learning_rate\")\n",
    "plt.show()\n",
    "\n",
    "# for iter in range(lf_df.shape[0]):\n",
    "#     leg='Combination '+str(iter+1)\n",
    "#     plt.plot(lf_df[iter,:],label=leg)\n",
    "# plt.title(\"Last 10 norm fifferences for every combination of learnig rates\")\n",
    "# plt.legend()\n",
    "# plt.show()           \n",
    "    \n",
    "# # fig, ax1 = plt.subplots(1,1)\n",
    "# # data = np.random.randint(0, 100, size=(10, 10))\n",
    "# # ax1.imshow(data, cmap='jet', interpolation='nearest')\n",
    "# # ax1.set_xticklabels(['', 0,10,20,30,40])\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py34]",
   "language": "python",
   "name": "conda-env-py34-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
