{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/miniconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=''\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rng\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import operator\n",
    "from utils.utils import tf_matmul_r, tf_matmul_l, tf_eigvecs, tf_eigvals\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p=784 #input size\n",
    "m=1000 # No of hidden units \n",
    "n=10 # Output size \n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "#Training parameters \n",
    "# iterations=3000\n",
    "# i=tf.Variable(0,name='loop_i')\n",
    "eta=tf.constant(0.00001,dtype=tf.float32)\n",
    "\n",
    "#Training data inputs\n",
    "'''x=tf.placeholder(tf.float64,[None,inshape], name = 'x')\n",
    "y=tf.placeholder(tf.float64,[None,outshape], name = 'y')\n",
    "'''\n",
    "x=tf.placeholder(tf.float32,[batch_size,784])\n",
    "y=tf.placeholder(tf.float32,[batch_size,10])\n",
    "\n",
    "#Scale weight initialization\n",
    "alpha0 = np.sqrt(2.0/p)\n",
    "alpha1 = np.sqrt(2.0/m)\n",
    "alpha2 = 1\n",
    "\n",
    "#parameters for feedback alignment\n",
    "A = tf.Variable(rng.randn(p+1,m)*alpha0, name=\"hidden_weights\", dtype=tf.float32)\n",
    "W = tf.Variable(rng.randn(m+1,n)*alpha1, name=\"output_weights\", dtype=tf.float32)\n",
    "B = tf.Variable(rng.randn(m+1,n)*alpha2, name=\"feedback_weights\", dtype=tf.float32)\n",
    "\n",
    "e0 = tf.ones([batch_size, 1], tf.float32)\n",
    "e1 = tf.ones([batch_size, 1], tf.float32)\n",
    "x_aug = tf.concat([x, e0], 1)\n",
    "\n",
    "\n",
    "h=tf.sigmoid(tf.matmul(x_aug,A))\n",
    "h_aug=tf.concat([h,e1],1)\n",
    "y_pred=tf_matmul_r(h_aug,W,B) \n",
    "\n",
    "trainable=[A,W]\n",
    "          \n",
    "e=y_pred-y\n",
    "loss = tf.reduce_sum(tf.pow(e, 2))/2\n",
    "grad_W=tf.gradients(xs=W,ys=loss)[0]\n",
    "grad_A=tf.gradients(xs=A,ys=loss)[0]\n",
    "\n",
    "delta_bp = tf.matmul(e, tf.transpose(W))[0,:]\n",
    "delta_fa = tf.matmul(e, tf.transpose(B))[0,:]\n",
    "\n",
    "alignment = tf.reduce_sum(tf.multiply(delta_fa,delta_bp))/tf.norm(delta_fa)/tf.norm(delta_bp)\n",
    "# eigs = tf_eigvals(tf.matmul(tf.transpose(B), W))\n",
    "\n",
    "new_W = W.assign(W - eta*grad_W)\n",
    "new_A = A.assign(A - eta*grad_A)            \n",
    "train_step = [new_W, new_A]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch no: 0 completed\n",
      "9999\n",
      "Epoch no: 1 completed\n",
      "9999\n",
      "Epoch no: 2 completed\n",
      "9999\n",
      "Epoch no: 3 completed\n",
      "9999\n",
      "Epoch no: 4 completed\n",
      "9999\n",
      "Epoch no: 5 completed\n",
      "9999\n",
      "Epoch no: 6 completed\n",
      "9999\n",
      "Epoch no: 7 completed\n",
      "9999\n",
      "Epoch no: 8 completed\n",
      "9999\n",
      "Epoch no: 9 completed\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "iteration= 10000\n",
    "epoch=10\n",
    "store_al=np.zeros((epoch,iteration))\n",
    "store_err=np.zeros((epoch,iteration))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch_no in range(epoch):\n",
    "        for idx in range(iteration):\n",
    "            (train_x, train_y) = mnist.train.next_batch(batch_size)\n",
    "\n",
    "#             sess.run(updatefa(A,W), feed_dict={x: train_x, y: train_y})\n",
    "            _,err,align = sess.run([train_step,loss,alignment], feed_dict={x: train_x, y: train_y})\n",
    "            if np.isnan(err)==True:\n",
    "                print(\"\\n\\tModel does not converge!!!\\n\")\n",
    "                break        \n",
    "            store_err[epoch_no,idx]=err\n",
    "            store_al[epoch_no,idx]=align\n",
    "        print(\"Epoch no: %d completed\"%epoch_no)\n",
    "#         print(idx)\n",
    "with open(\"MNIST-FA.pkl\",'wb') as f:\n",
    "    pickle.dump([store_err,store_al,iteration,epoch],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
